# aws-devops-professional
## Table of Contents
 - [SDLC automation](#sdlc-automation)
 - [Configuration Management and IaC](#configuration-management-and-iac)
 - [Resilient cloud solutions](#resilient-cloud-solutions)
 - [Monitoring and Logging](#monitoring-and-logging)
 - [Incident and Event Response](#incident-and-event-response)
 - [Security and Compliance](#security-and-compliance)
 - [Other Services](#other-services)
 - [Filling the gap](#filling-the-gap)

## SDLC Automation
 - CI/CD: having our code in a repo and deploy it onto AWS `automatically, the right way, making sure it's tested before being deployed, with possibility to go into different stages(dev, test, staging, prod), with manual approval where needed`.
   - CI: dev push code to the repo, a test/build server checks the code and gives the feedback to the dev.
   - CD: ensure software can be released reliably when needed and deployment happens ofter and quickly
   - tech stack: (code: codeCommit/github/...) -> (build & test: codebuild/jenkins/...) -> (deploy: codeDeploy) -> (provision: EC2/on-prem/lambda/ECS/...)
     - use `Elastic Beanstalk` to cover `deploy` and `provision` stages
     - use `codePipeline` to manager the whole process
 - CodeCommit: version control/ using GIT/ Private repo/ no size limit/ fully managed, HA/ integrate with various CI tools/ Security: ssh key & https(authentication); IAM policies(authorization); encryption(automatically with MKS at rest, in transit, https or ssh); using IAM role and STS (cross-account access) 
 - CodeCommit - advanced:
   - `monitoring with eventbridge(near real-time)`
   - `migrate git repo to codeCommit`
   - cross-region replication: use `eventbridge` to listen to events, then invoke ECS task to replicate to another repo in another region. Use case: lower latency for global devs, backups,...
   - branch security: using IAM policies to restrict users to push or merge code to a specific branch. `note: resource policy not supported yet`
   - pull request approval rules: specify a pool of users to approve PR and the number of approvals. specify IAM principal ARN(users, roles,groups). then setup `Approval rule templates` 
 - CodePipeline: visual workflow to orchestrate your CI/CD. `source`,`build`,`test`,`deploy`,`invoke`. Each stage can have sequential actions and/or parallel actions. Manual approval can be defined at any stage. `Artifact`: each stage will output artifacts and put them into S3 bucket, then will be put into next stage. `Troubleshooting`: use cloudwatch events for failed pipelines or cancelled stages. if a stages failed, it can be seen in the console. if pipeline cannot perform an action, check the IAM role. Also cloudtrail can be used to audit aws api calls.
 - CodePipeline - extra:
   - events vs webhooks vs polling
   - manual approval --> two permissions: `get pipeline`, `put approval result`
 - CodePipeline - cloudformation integration: `cloudformation deploy action` can be used to deploy aws resources. to deploy resources across accounts or regions, using `stacksets`. Using `CREATE_UPDATE` mode for existing stacks, and `DELETE_ONLY` to delete an existing stack. `workflow`: codebuild-->cfn(create_update)-->codetest-->cfn(delete_only)-->cfn deploy prod infra(create_update)
   - action modes: create/replace/execute a change set; create/update/delete/replace a stack
   - template parameter override
 - CodePipeline - advanced:
   - best practices: one codepipeline, one codedeploy, parallel deploy to multiple deployment groups; parallel actions used in a stage using `RunOrder` (similar to github action without `steps`); deploy to a `pre-prod` before deploying to prod
   - with EventBridge: detect and react to any failures in any stage (such as invoke lambda or send SNS)
   - Invoke actions: lambda & step functions
   - multi-region: actions in the pipeline can be in different regions; s3 artifact stores must be provisioned in each region(must have read/write); codepipeline will handle copying artifact across regions automatically. (multiple templates may need to be created for multiple regions)
 - CodeBuild:
   - source: codeCommit, s3, bitbucket, github
   - build instructions: `buildspec.yaml` or insert manually in console
   - output logs: can be stored in s3 & cloudwatch logs
   - monitor using cloudwatch metrics
   - eventbridge to detect failed builds and trigger notifications
   - cloudwatch alarms to notify if you need `thresholds` for failures
   - build projects can be defined within codepipeline or codebuild
   - `buildspec.yml`:
     - located at the root dir
     - env: `variables`,`parameter-store`,`secrets-manager`
     - phases: `install`,`pre_build`,`build`(build commands),`post_build`
     - artifacts: upload to s3(using KMS)
     - cache: files to cache(usually deps) to s3 for future build
   - codebuild - local build: codebuild agent for deep troubleshooting beyond logs
   - codebuild - inside VPC: by default, codebuild runs outside the VPC. you can specify: VPC id, subnet id, security group id, then your build can access resources in your vpc. use case: integration tests, data query, internal load balancers...
 - CodeBuild - advanced:
   - env variables: default env variables, custom env variables(static--defined at build time, dynamic--parameter store, secrets manager)
   - security: codebuild service role
   - build badges: support codecommit, github, bitbucket. available at branch level
   - triggers: eventbridge, lambda, github(webhook)
   - validate pull requests
   - test reports: various tests with any test framework. in `buildspec.yml`, add a `report group` in the `reports` section
 - CodeDeploy: deploy new versions to ec2, on-prem, lambda, ecs; automated rollback capability in case of failures or trigger cloudwatch alarm; gradual deployment control; a file `appspec.yml` configuring the deployment
   - ec2/on-prem: perform in-place or blue/green deployments; must run codedeploy agent on the target instances; define deployment speed: allatonce, halfattime, oneatatime, custom.
   - in-place deployment: update a certain part at a time
   - blue/green deployment: create a new identical group but with new version, then update
   - codedeploy agent: must on ec2 as pre-requisites, using ssm to install and update automatically. must have permissions to access s3 to get deployment bundles.
   - lambda: help automate traffic shift for lambda aliases; feature integrated with SAM. linear: grow traffic every N min until 100%. canary: try X percent then 100%. AllAtOnce
   - ecs: only support blue/green deployment. linear, canary, allatonce
 - CodeDeploy - EC2 deep dive: use ec2 tags or asg to identify instances you want to deploy to. Deployment Hooks: certain scripts run by codedeploy on each ec2 instances. process(using load balancer): block traffic, app stops, install , restart app, validate service, allow traffic.
   - deployment hooks examples: beforeinstall(such as decrypting files, creating backup), afterinstall(such as configure app), applicationstart, validateservice, beforeallowtraffic(such as perform health checks before registering it to load balancer)
   - blue/green deployment(must have a load balancer): manually mode: provision blue and green and identify by tags. automatic mode: new asg is provisioned
   - blue/green instance termination: blueinstanceTerminationOption(whether delete blue after deployment), action terminate(specify wait time), action keep alive(instances keep running but deregisterd from ELB and deployment group)
   - deployment hooks: some for v1, others for v2
   - deployment configurations: specify number of instances remain available during the deployment, using pre-defined configs: allatonce, halfatatime, oneatatime or custom
   - trigger: send events to SNS topic
 - CodeDeploy - ECS deep dive: automatically handle new ecs task definition to ecs service. only support blue/green, ecs task definition and container image must be ready, the task definition and load balancer info are specified in `appspec.yml`. no codedeploy agent required
   - deployment to ecs: linear, canary, all at once. can define a second ELB test listener to test the green before traffic is rebalanced.
   - deployment hooks: all lambda functions. such as `afterAllowTestTraffic`: perform health check and trigger a rollback if it is failed
 - CodeDeploy - lambda deep dive: a lambda alias pointing to v1, then create a v2 and specify the version info in the `appspec.yml`, then codedeploy updates the version by making lambda alias pointing to v2. no codedeploy agent required.
   - only blue/green. linear, canary, all at once
   - hooks
 - CodeDeploy - rollbacks & troubleshooting:
   - rollback: redeploy the previous version (automatically when cloudwatch alarm threshold breach, or manually)
   - disable rollbacks
   - when rollback, a new deployment will be created(not restore the version)
   - troubleshooting: exception`InvalidSignatureException`: when the date and time on ec2 instance is not set correctly, they might not match the signature date of your deployment request, which codedeploy rejects.
   - when deployment or all lifecycle events are skipped(ec2/on-prem) with errors: `too many individual instances failed deployment`, `too few healthy instances for deployment`, `some instances are experiencing problems`. Reasons: no codedeploy agent, service role, codedeploy agent with http proxy, date and time mismatch between codedeploy and agent.
   - when an asg is performing scale-out operation, a new instance with v1 will be created, but by default, codedeploy will perform a follow-on deployment to update the version.
   - when failed `allowTraffic` in blue/green with no error, check out ELB health check and correct it.
 - CodeArtifact: software packages or dependencies. store and retrieve them is called artifact management. work with common package management tools, such as maven, gradle, npm, pip. devs and codebuild can retrieve packages from codeartifact
   - eventbridge integration
   - resource policy used for cross-account premissions
 - CodeArtifact - upstream repositories & domains:
   - upstream repos: a codeartifact repo can have other codeartifact repos as upstream repos, so that a single repo endpoint is created.
   - external connection: a codeartifact repo and an external repo. 1-to-1
   - retention: upstream repo change will affect downstream, but we can delete it and update the package in the downstream. all intermediate repos do not keep the package, only the ones connect to the client and the external repo
   - domain: deduplicate storage by making a shared storage, thus we have fast copying, easy sharing across repos and teams. apply resource-based policy 
 - CodeGuru:
   - ML-powered service for code reviews and app performance recommendations: reviewer: static code analysis(development). profiler: recommendations about app performance during runtime(production)
 - CodeGure - extra:
   - codeguru reviewer secrets detector: identify secrets in your code, and suggest remediation with secrets manager.
   - codeguru profiler -extra: `function decorator @with_lambda_profiler`, or enable it through aws console
 - EC2 image builder: used to automate the creation of VM or container images(create, maintain, validate, and test ec2 ami). can be on a schedule. free service and can publish ami to multiple regions and multiple accounts.
 - EC2 image builder - extra: sharing using Resource access manager(RAM) to share images, recipes and components across aws accounts or through aws organization. tracking latest ami (using parameter store to store the latest ami version or id)
 - AWS amplify: elastic beanstalk for web and mobile apps
 - AWS amplify - extra: CD: connect to codecommit and have one deployment per branch(dev, prod), connect your app to a custom domain via route53

## Configuration Management and IaC
 - Cloudformation - overview: declarative way to provision aws infras
   - benefits:
     - Infrastructure as code
     - cost: resources within the stack have an identifier so that we can see how much does the stack costs you; estimate the costs of resources using cloudformation template; saving strategy: in dev, we can delete and recreate resources at certain time period.
     - productivity: declarative programming
     - seperation of concern: such as vpc stack, network stack, app stack
     - no need to re-invent the wheel: checkout existing templates, and docs
   - workflow: create and upload a template to s3 bucket, then cloudformation will create the stack, to update the stack, we have to upload a new version. when deleting a stack, all artifacts created by cloudformation will be deleted.
   - deploy cloudformation template: manual way & automated way
 - Cloudformation - create/delete stack
 - cloudformation:
   - resources: mandatory; the resource types identifiers are of the form: `service-provider::service-name::data-type-name`. **note:** for creating a dynamic number of resources, we can use cloudformation Macros and Transform. and using cloudformation resources to provision custom resources.
   - parameters: provides a way to insert inputs for your template. use cases: reuse the template or some inputs cannot be determined ahead of time. when to use parameters: if the certain config will likely change in the future, then make one. for the types of parameters: such as allowedValues or noEcho. to refer to a parameter, use the intrinsic function `!Ref <parameter name>` or `Fn::Ref`
     - Pseudo parameters: Aws offers a bunch of pseudo parameters by default which can be used at any time. Such as `AWS::AccountId`,`AWS::Region`,`AWS::StackId`,`AWS::StackName`,`AWS::NotificationARNs`,`AWS::NoValue` 
   - mappings: fixed variables(all possible values are known in advance), use the intrinsic function `Fn::FindInMap`(!FindInMap [<map-name>, <path>,... ]). use parameter when values are user-specific.
   - outputs & exports: the best way to perform collaboration cross stacks, using `Export: <name for the outputs> in the Outputs section`, and use the intrinsic function `Fn::ImportValue` to refer to the outputs, `!ImportValue <name of the outputs>`
   - conditions: used to control of the creation of resources or outputs based on a condition. to create a condition: `<condition name>: !Equals [!Ref envType, prod]`(and, equals, if, not, or...). to use a condition, `add a condition: <condition name> to the resource`.
   - intrinsic functions: Ref, Base64, GetAtt, FindInMap, ImportValue, Condition functions, ...
   - rollbacks:  when stack creation fails: by default, rolls back, or to disable rollback and troubleshoot. when update fails: by default, rolls back, able to see in the log of what happened. when rollback fails, fix resources manually then issue `continueUpdateRollback` api call from console/cli 
   - service role: IAM roles that allows cloudformation to create/update/delete resources on your behalf even if you don't have permission to work with the resources in the stack. the user must have a `iam::PassRole` permissions
   - capabilities: `CAPABILITY_NAMED_IAM` and `CAPABILITY_IAM`: iam-related resources; `CAPABILITY_AUTO_EXPAND`: needed when template include Macros or nested stacks to perform dynamic transformations. `InsufficientCapabiiltiesException`: if the capabilities haven't been acknowledged when deploying a template.
   - deletion policy: by default, deletionPolicy=Delete. **note:** s3 bucket won't be deleted if it's not empty.
     - deletionPolicy - retain: such as dynamodb table
     - deletionPolicy - Snapshot: create a snapshot before deleting the resource
   - stack policy: during stack update, all update action are allowed on all resources. a stack policy defines update actions allowed on certain resources during the stack updates (`Allow`/`Deny`)
   - termination protection:  applied on the entire stack (differ from deletion policy)
   - custom resources: used to define resources not supported by cloudformation, or on-prem, 3rd-party resources, or having custom scripts to run through create/update/delete through lambda functions(such as empty an s3 bucket before deleting it). `AWS::CloudFormation::CustomResource` or `Custom::MyCustomResourceTypeName`(recommended). backed by lambda or SNS topic.
     - define a custom resource: `serviceToken`: where cloudformation sends requests to, such as lambda ARN or SNS ARN(must be in the same region). input data parameters.
   - dynamic references: refer to the external values stored in SSM parameter store and secrets manager. Supports: ssm/ssm-secure/secretsmanager, `{{resolve:<service-name>:<reference-key>:<version>}}`
     - option1 - `ManageMasterUserPassword: true`: create admin secret implicitly for rds, aurora
     - option2 - `dynamic reference`: 1. create secret 2. reference secret in rds instance. 3 secretRDSAttachment: link secret to the db instance for rotation.
   - user data: a script to be executed during the launch of ec2 instance for the first time. **note:** the important thing to pass is the entire script through the function `Fn::Base64`. Good to know, the use data script log is in the `/var/log/cloud-init-output.log`
   - cfn-init: `cloudformation helper scripts: python scripts that comes on AMIs or installed using yum on non-amazon amis`. `under metadata section, aws::cloudformation::init(config: packages, files, commands, services)`
   - cfn-signal & wait condition: run `cfn-signal` after `cfn-init`. we need waitCondition: block the template until there's a signal from `cfn-signal`. we attach a `<sample wait condition>.CreationPolicy`, we can define a `Count > 1` if we need more signals.
   - cfn-signal failures: wait condition didn't receive the required number of signals from resource, such as ec2 instance:
     - ensure the AMI has helper scripts installed, or download them to the instance.
     - verify the `cfn-init`, `cfn-signal` run successfully, viewing the logs at `/var/log/cloud-init.log`,`/var/log/cfn-init.log`.
     - make sure to disable the `rollback on failure`, otherwise, all logs and the instance will be delted
     - make sure the instance has the access to the internet if it's in a vpc or even a private subnet 
   - nested stacks: allows you to isolate repeated patterns/common parts. cross stack vs nested stacks
   - depends on: `DependsOn`, applied automatically when using `!Ref` or `!GetAtt`
   - troubleshooting: `delete_failed`: such as delete s3 bucket when it's not empty, or use custom resources with lambda functions to automate some actions like emptying s3 bucket; security groups cannot be deleted until all ec2 instances in the group are gone; or check the deletionpolicy(if it's retain-->skep deltion). `update_rollback_failed`: can be caused by resources changed outside of cloudformation, insufficient permissions, asg that doesn't receive enough signals... Manually fix the error, then `continueUpdateRollback`. `stackset--troubleshooting`: insufficient permission in a target account for creating resources specified in your template, or trying to create a global resource but not unique, or admin account has not a trusted relationship with target accounts, or reached the limit or quota in the target account(too many resources) 
   - changesets: it won't tell if the update will be successful, but provide a preview of what will be changed before applying them. for nested stacks, you will see the changes across all stacks
   - cfn-hup: can be used to tell your ec2 instance to look for metadata changes every 15 min and apply the metadata config again, it replies on `cfn-hup` config: `/etc/cfn/cfn-hup.conf` and `/etc/cfn/hooks.d/cfn-auto-reloader.conf`
   - drift: cloudformation drift detect drift on entire stack or individual resource. or perform drift detection on stackset. any changes made outside of cloudformation is considered drifted. (drift detection feature has to be enabled manually)
 - stacksets - warning: using admin account to create/update/delete stackset, once update or delete stackset, it will applied into all accounts or regions/ or all accounts of AWS organizations.
   - permission models: `self-managed`: create iam roles for admin and target and build a trust relationship. `service-managed`: utilizing AWS organization(enable all features and trusted access). deploy stack to new account in organization automatically. can delegate stacksets admin to memeber account. again, trusted access in organization must be enable before delegating admin.
 - cloudformation - stacksets (create / update / delete)
 - service-catalog: admin users create a portfolio(collection of products--templates) with iam permissions, then users with proper iam permissions will launch certain template. help ensure consistency, governance, compliance. integration with `self-service portals` such as ServiceNow.
 - servie-catalog - extra:
    - stack set constraints: accounts, regions, permissions
   - launch constraints: iam role assigned to a template(product), example: end user has only access to the catalog, others permissions required are attached to the launch constaint iam role. iam role mush have permissions: cloudformation(full access), aws services in the template, s3 bucket which contains the cloudformation template(read access)
   - CD pipeline(syncing with codeCommit): codeCommit(mapping.yml-->product<->template)-->lambda(checkout template)-->update/create service catalog(according product)
 - Elastic Beanstalk
   - overview: application, application version, environment
   - HA environment: 
   - deployment modes: All at once / Rolling / Rolling with additional batches( old app is still available while spinning up new instances) -- additional cost / immutable (spins up new instances in a new asg, deploys version to these instances, then swaps all the instance when everything is healthy) -- high cost / (blue/green): create a new env and switch over when ready by utilizing Route53 to redirect a portion of traffic to the new env, then swap urls when done with env test / traffic splitting(canary testing) - send a small % of traffic to new deployment (a temporary asg of new version with a small portion of traffic, then migrate them to the main asg and terminate old version, otherwise automatically rollback)
   - extra: web server vs worker env(long-term task); notification via eventbridge
 - serverless application model(SAM)
   - overview: a framework for developing and deploying serverless apps. can use codedeploy to deploy lambda functions and help run lambda, api gateway, dynamodb locally.
     - recipe:
     - workflow: app code + sam template --> app code + cfn template --> sam package or zip cfn package to s3 bucket --> deploy to cfn to build stack
     - cli debug: locally build, test, debug serverless apps. providing a lambda -like execution env locally. sam + aws toolkit --> step through and debug your code (an IDE plugin which allows to test lambda functions using aws sam)
   - with CodeDeploy: natively use codedeploy; traffic shifting feature; pre and post traffic hooks; easy & automated rollback using cloudwatch alarms
     - sam and codedeploy:
       - autoPublishAlias: detect changes; create and publish a updated version; point the alias to the new version.
       - deploymentPreference: canary / linear / allAtOnce
       - alarms: alarms that trigger a rollback
       - hooks: pre and post traffic shifting lambda functions to test your deployment
 - cloud development kit (CDK): using programming languages to define infras `constructs`, which will be transformed into cloudformation template. thus, you can deploy app code and infra code together: great for lambda function and docker containers in ecs/eks.
   - cdk + sam: use sam cli to test your cdk apps
 - step functions: model your workflow as state machine(start workflow with SDK call, API gateway, eventBridge).
   - task states: invoke one aws service, or run an activity
   - states: choice state / fail or success state / pass state / wait state / map state / parallel state
 - AppConfig: could have config outside of your app; deploy dynamic config changes to your app independently(no need to restart the app). feature flags, app tuning, allow/block listing... gradually deploy the config changes and rollback if issues occur. validate config changes before deployment using: json schema(syntactic check) or lambda function - run code to perform validation (semantic check)
 - system manager (SSM): help manage ec2 and on-prem systems at scale; patching automation for enhanced compliance; work for windows and linux; integrate with cloudwatch and aws config; easily detect problems and get insights of your infra.
   - features: Resource groups, shared resources--documents, change management--automation,maintenance windows, application management--parameter store, node management--inventory,session manager, run command, state manager, patch manager. need a ssm agent installed on the target machine(such as Amazon linux 2 ami), check the ssm agent installation as well as iam role which allows ssm actions
 - AWS tags & SSM resource groups: add key-value tags to aws resources, which used for resources grouping, automation, cost allocation... better to have too many tags than too few. / create, view or manage logical group of resources thanks to tags, allows to create logical groups of resources such as apps, different layers of app stack, prod vs dev envs. regional service. work with ec2, s3, dynamodb, lambda, etc.
 - SSM documents(center of SSM) & SSM run command:
   - documents: define parameters, actions, many existing documents. these documents can be used in other SSM features.
   - run commands: execute a document, across multi-instances, rate control/error control, integrated with iam and cloudtrail, no need for ssh, output can be shown in the console, s3 bucket or cloudwatch logs, send notifs to SNS abou the command status, can be invoked by eventbridge.
 - SSM automations
   - overview: simplify common maintenance and deployment tasks of ec2 instances and other aws resources. `automation runbook`: ssm documents of type automation, pre-defined runbooks or custom runbooks. can be triggered by aws console, SDK, cli, eventbridge, maintenance windows, aws config remediation. 
   - use case
 - SSM parameter store: secure storage for config and secrets/ version tracking/ eventbridge/ cloudformation/ iam/ kms/ serverless,scalable,durable,easy SDK./ store hierarchy./ parameter policies(advanced tier): set TTL to a parameter, assign multiple policies at a time.
 - SSM patch manager and maintenance windows:
   - patch manager: automates patching process, os(linux, macos, windows) updates, app updates, security updates... support both ec2 and on-prem, patch on-demand or scheduled using `maintenance windows`, scan instances and create patch compliance report and send to s3 bucket. `patch baseline`: what should or should not be installed. `patch group`: associate a set of instances with a specific patch baseline. instances should be defined with the tag key `Patch Group`. one instance only attached to one group. one group only registered with one baseline. `pre-defined patch baseline`: `AWS-RunPatchBaseline(ssm document)`, `custom patch baseline`
   - maintenance windows: defines a schedule for when to perform actions on your instances. `schedule`, `duration`, `set of registered instances`, `set of registered tasks`
 - SSM session manager
   - overview: allows to start a secure shell on your ec2 and on-prem. no need for ssh access, bastion hosts or ssh key. support multi-os, logs can be sent to s3 bucket or cloudwatch logs. cloudtrail can be used to intercept `StartSession` events. `IAM permissions`: control access. optionally, restrict commands a user can run.
   - with VPC endpoints: to connect to ec2 instances in a private subnet without internet access: vpc interface endpoint(for ssm), vpc interface endpoint(for ssm session manager), vpc interface endpoint(optional kms), vpc interface endpoint(optional cloudwatch logs), vpc gateway endpoint(optional - s3 bucket) **note**: for s3 gateway endpoint, update route tables
 - ~SSM cleanup~
 - SSM default host management configuration (DHMC): when enabled, ec2 instance will be config as managed instance without the use of `ec2 instance profile`. `Instance identity role`: a type of iam role with no permissions beyond identifying the instance to aws service(such as ssm). ec2 instances must have `IMDSv2 enabled` and `SSM Agent installed`. session manager, patch manager and inventory enabled automatically. must be enabled per region. ssm agent up to date automatically.
 - SSM hybrid environments: setup ssm to manage on-prem servers, IoT devices, edge devices and virtual machines(for hybrid managed nodes, use the prefix 'mi-'). workflow: hybrid activation --> activation code and Id --> install ssm agent --> registered with activation code and id(could use API gateway, lambda and ssm to automate the process)
 - SSM with IoT greengrass: IoT Core runs in the cloud and Greengrass runs at the edge (usually). IoT Core is a cloud service but Greengrass is an edge runtime. to manage iot greengrass core devices using ssm, install ssm agent on core devices and add permissions to the `token exchange role`(IAM role for the IoT core device) to communicate with ssm, support all ssm features. use cases: update and maintain os and software across a fleet of greengrass core devices.
 - SSM compliance: scan managed nodes for patch compliance and config inconsistencies. display current data about `patched in patch manager`, `associations in the state manager`. can sync data  to s3 bucket using `resource data sync` and analyze using athena and quicksight. can collect and aggregate data from multiple accounts and regions. can send compliance data to `security hub`. 
 - SSM opsCenter: allows to view, investigate, and remediate issues in one place(no need to go to different aws services). security issues(security hub), performance issues(dynamodb throttle), failures(asg failed launch instance),... reduce meantime to resolve issues. support both ec2 instances and on-prem nodes. `OpsItems`: issues or interruptions; event, resource, aws config changes, cloudtrail logs, eventbridge... `provide recommended runbooks to resolve the issue` 
 - AWS opsWorks
   - get started (1 & 2): `opswork stacks`(exam required), `opswork for chef automate`, `opswork for puppet enterprise`. `opswork stacks`: a configuration management service that helps you build and operate highly dynamic apps and propagate changes instantly. a stack is a set of layers, instances and related aws resources whose configuration you want to manage together / `time-based instances` & `load-based instances` / Apps / deployments / monitoring / resources / permissions / tags
   - lifecycle events(most important): each layer has a set of five lifecycle events(`setup`(includes `deploy`),`configure`,`deploy`,`undeploy`,`shutdown`(happen when shutting down an instance but before it's terminated completely)), each of which has an associated set of recipes that are specific to the layer. for `configure` events, it occures on all instances when one instance enters or leaves online state; attach or detach Elastic IP to/from an instance; attach or detach ELB to/from a layer. **note**: `configure` happens on all instances, others are instance-specific
   - cloudwatch events integration: (opswork has auto-healing feature, to listen to that change happening, create a rule on eventbridge)
   - summary
## Resilient cloud solutions
 - Lambda
   - versions and aliases: `$LATEST`, versions(code + configuration, has their own ARN) are immutable, and each version can be accessed as well as the `$LATEST`. Aliases are pointers to different versions, which also are mutable and enable `canary` deployment. they have their own ARN and cannot reference other aliases.
   - environment variables: key-value pair, lambda has its own env variables as well, can be encrypted by kms, lambda service key or CMK
   - concurrency: `The default concurrency limit across all functions per region in a given account is 1,000`. can set `reserved concurrency` at the function level. each invocation will trigger a `throttle`(synchronous--> throttleError 429; asynchronous--> retry automatically and then go to DLQ), open a support ticket if you need a higher limit.
     - concurrency and asynchronous invocations: throttleErrors(429) or system errors(500), events will be returned to the queue and try to run the function again up to 6 hrs. the retry intercal increases exponentially from 1 sec to 5 min at maximal.
     - cold starts & provisioned concurrency: first request served by new instances has higher latency than the rest. Using provisioned concurrency to reserve a certain amount of concurrency for the function so that cold start never happen. Application auto scaling can manage concurrency(schedule or target utilization)
     - reserved and provisioined concurrency: reserved-->This represents the maximum number of concurrent instances allocated to your function. When a function has reserved concurrency, no other function can use that concurrency. Configuring reserved concurrency for a function incurs no additional charges. provisioned-->This is the number of pre-initialized execution environments allocated to your function. These execution environments are ready to respond immediately to incoming function requests. Configuring provisioned concurrency incurs additional charges to your AWS account.
   - file systems mounting: lambda function can access EFS file system if they run in a VPC. config lamdba to mount EFS file system to local directory during initialization. must leverage EFS access points. limits: EFS has connection limits and connection burst limit.
   - cross-account file systems mounting: lambda in VPC A needs several permissions -- VPC peering -- EFS file system in VPC B + access point  needs setup a EFS policy to allow account A.
 - API Gateway
   - overview: fully-managed, support websocket, api versioning, different env, authentication & authorization, api keys and handle request throttling, swagger/open api import to quickly define apis, transform and validate requests and responses, generate sdk and api specification, cache api responses.
     - integrations: lambda function, HTTP(internal HTTP API on-prem, ALB), aws service
     - endpoint types: edge-optimized(default, for global clients), regional, private(only be accessed in your VPC).
     - security: user authentication--> IAM roles(internal apps), cognito(external users), custom authorizer; custom domain name https through ACM(aws certificate manager)--> for edge-optimized, certificate must be in `us-east-1`, for regional, certificate must be in the api gateway region; must setup CNAME or A-alias record in route53.
   - stages and deployment: when making changes to api gateway, you have to deploy it to `stages`(as many as you want, like dev, test, prod), each stage has its own config and can roll back to a history deployment.
     - stage variables: like env variables for api gateway, can be used in lambda function ARN, http endpoint, parameter mapping template. use cases: config http endpoints your stages talk to; pass config parameters to lambda through mapping templates. stage variables to passed to the `context` object in the lambda. format: `${stageVariables.variableName}`
     - stage variables & lambda aliases
   - Open API: using API definition as code to define rest apis. import exsiting openapi3.0 spec to api gateway(method,method request,integration request,method response,aws extensions for API gateway and setup every single option), can export current api as openapi spec. using openapi we can generate sdk for our apps.
     - rest api --request validation -- openapi (setup request validation by importing openapi definitions file): `params-only` or `all validator on the post/validation method`  
   - caching: default TTL(300 secs), can be defined `per stage`, possible to override cache settings `per method`, can be encrypted, capacity(0.5GB--237GB), expensive-->use it in prod.
     - cache invalidation: flush entire cache instantly. `header:Cache-Control:max-age=0`(with proper IAM authorization). any client could invalidate the cache if no invalidateCache policy imposed or not choosing `require authorization checkbox in the console`
   - canary deployment: possible to enable canary deployments for any stage(usually prod). metrics and logs separate(for better monitoring). possible to override stage variables. this is blue/green deployment with lambda and api gateway
   - monitoring, logging and tracing:
     - cloudwatch logs: contain request/response body, enabled at stage level, and can override settings on a per api basis
     - x-ray: enable to trace to get some extra info about the requests in the api gateway. x-ray api gateway and lambda gives the full picture.
     - cloudwatch metrics: metrics are stage-specific, possible to enable detailed metrics. `CacheHitCount` & `CacheMissCount`: efficiency of cache. `Count`: total number of requests in a given period. `integrationLatency`: time between api gateway relay a request to the backend and api gateway receives the response from the backend. `latency`: time between api gateway receives a request from client and it returns a response to the client (includes integration latency and other API gateway overhead). 4xx client error, 5xx server error.
     - api gateway throttling: account limit, soft limit (10000 rps), error 429 too many requests. can set stage limit & method limits to improve performance. or define `usage plans` to throttle per customer
     - api gateway errors: 4xx: 400, 403, 429; 5xx: 502(bad gateway, incompatible output from a lambda proxy integration backend or out-of-order invocations due to heavy loads), 503(service unavailable), 504(integration time-out 29 secs)
 - ECS:
   - overview: `ec2 launch type`, launch docker containers=launch `ecs task`, you must provision and maintain the infra(ec2 instances), each ec2 instance must have an ecs agent, aws takes care of containers. `fargate launch type`: no need for managing infra, is serverless, just need to create task definitions. aws run ecs tasks based on cpu/ram you need. to scale, just increase the number of tasks
     - IAM roles: `ec2 instance profile`(only for ec2 launch type): used by ecs agent to talk to ecs, ecr, cloudwatch logs, secrets manager or ssm parameter store. `ecs task role`: defined in the `task definition`, allow each task to have a specific role to talk to other services.
     - load balancer integrations: ALB suits for most cases, NLB for high throughput use cases.
     - data volumes(EFS): works for both ec2 and fargate launch types. used for persistent multi-az shared storage for your containers. **note**: s3 cannot be used as file system.
   - auto scaling: `ecs service auto scaling`: `aws application auto scaling`, three metrics: ecs service average cpu utilization, memory utilization, ALB request count per target. tracking strategies: target tracking(cloudwatch metric), step scaling(cloudwatch alarm), scheduled scaling(predictable changes). ecs service auto scaling!= ec2 auto scaling. `ec2 launch type -- auto scaling ec2 instances`: 1 auto scaling group(based on CPU usage), 2 ecs cluster capacity provider: paired with asg, used to scale infra for ecs tasks automatically.
   - solution architectures: invoked by eventbridge, eventbridge schedule, SQS, intercept `stopped tasks` using eventbridge
   - logging: logging with `awslogs` driver, need to turn on `awslogs` log driver, and config `logConfiguration` parameters in the task definition. `fargate launch type`: task execution role must have required permissions. `ec2 type`: use `cloudwatch unified agent and ecs container agent`, enable `ECS_AVAILABLE_LOGGING_DRIVERS` in `/etc/ecs/ecs.config`. `logging with sidecar container`: used for collecting logs and send to cloudwatch logs
 - ECR: images storages service, access controlled by iam, support vulerability scan,versioning...
 - ECR - extra: lifecycle policies, uniform pipeline(pull images regardless programming languages)
 - EKS: support ec2 and fargate launch types. use case: if k8s already used on-prem or in other cloud. `node types`: `managed node group`, `self-managed nodes`, `aws fargate`. `data volumes`: EBS, EFS, FSx for lustre, FSx for netapp ontap
 - EKS - logging: send logs to cloudwatch logs(control plane logs: API server, audit, authenticator, controller manager, scheduler. can select which ones to send to cloudwatch). `nodes & container logs`: use `cloudwatch agent` to send metrics, use `fluent bit` or `fluentd` to send logs to cloudwatch logs. container logs path: `/var/log/containers`, use `cloudwatch container insights` to get monitoring solution for nodes, pods, tasks and services.
 - Amazon Kinesis: collect, process,and analyze streaming data in real-time.
 - kinesis data streams
   - overview: up to 1 yr retention, can replay, cannot be deleted, ordering(partition key), producers: SDK, kpl, kinesis agent, consumers: kcl, SDK, lambda, firehose, data analytics.
     - provisioned mode: each shard 1mb/s in, 2mb/s out
     - on-demand mode: default 4mb/s in , automatically scale based on throughput peak observed during last 30 days
     - security: iam policies, https, kms, or cmk, vpc endpoints for kinesis to access vpc, monitor api calls using cloudtrail
   - consumers scaling: `GetRecords.IteratorAgeMilliseconds`(cloudwatch metric): the difference between current time and the time when last record was written via `GetRecords` call. `IteratorAgeMilliseconds` > 0, then we are not processing the records fast enough
 - kinesis data firehose: fully-managed , destinations: aws redshift, s3, opensearch; 3rd party; custom http endpoint. near real-time: write data in batch, buffer: 1mb minimal, interval(0 -- 900 secs). support many data format, transformation, or custom transformation using lambda, can send failed or all data to backup s3 bucket
 - kinesis data analytics
   - overview: `sql applications`--> real-time analytics on kds & kdf using SQL, add reference data from s3 to enrich streaming data. fully-managed. output: kds & kdf. use cases: time-series analytics, real-time dashboard, real-time metrics. `for apache flink`(was mks managed streaming kafka)--> use flink(java,scala or sql) to process and analyze streaming data. flink does not read from firehose (use kinesis analytics instead)
   - using ML: `RANDOM_CUT_FOREST`: sql function used for anomaly detection, using recent history to compute model. `HOTSPOTS`: locate and return info about relatively dense regions in your data.
 - Route53
   - overview: fully-managed, authoritative, health check, only service with 100% sla. `records`: type: A/AAAA/CNAME, domain name, value, routing policy, TTL. `hosted zones`(container for records): public hosted zones, private hosted zones(how to route traffic within one or more vpcs). $0.5 per month per hosted zone 
   - routing policies:
     - weighted: control the % of the requests that go to each specific resource. weights don't need to sum up to 100, can be associated with health check, dns records must have the same name and type. assign weight 0 to a record to stop sending traffic. if all records are weight 0, then all records will be returned equally.
     - latency: direct traffic to the least latent for the clients. latency is based on traffic between users and aws regions, can be associated with health check.
     - failover: active-passive
 - RDS read replicas vs multi-AZ:
   - `read replicas for read scalability`: up to 15, replication is async, can be promoted to their own DB having their own lifecycle, apps must update the connection string. `use cases`: create a replica for reporting app or data analytics. only for read operations. `network cost`: same region is free, cross-region is not free.
   - multi-az: disaster recovery. sync replication. one dns name -- automatically failover. no manual intervention, not for scaling. read replica can be setup as multi-az for disaster recovery.
   - from single az to multi az: zero downtime(no need to stop db). click on 'modify' button, and a snapshot is taken, a new db is restored from the snapshot, then synchronization is established between two db instances.
 - Aurora - extra: `auto scaling`: writer endpoint and reader endpoint. `global aurora`: `aurora cross region read replicas` or `aurora global database`(recommended): 1 primary region(read/write), and up to 5 secondary regions(read-only), each of which can has up to 16 read replicas. promoting another region(for disaster recovery) has a RTO of < 1 min. typical cross-region replication takes less than 1 sec
   - unplanned failover: aurora endpoint stored in ssm parameter store, and use a lambda to health check global aurora, then send a notif to admin if it's failed, then admin will promote another region and update the endpoint in the ssm parameter store.
   - global application: each region has a local aurora db and a aurora global shared datasets.
 - elasticCache
   - overview: serverless service, support redis or Memcached, help load off read intensive workloads of db, or make your app stateless. `use case`: db cache, must have an invalidation strategy to make sure only the most current data is used in there. user session store.
     - redis vs Memcached: redis--> data durability, memcached--> pure data caching, multi-threaded architecture
   - redis cluster modes:
     - cluster mode disabled: one shard, all nodes have all the data, one primary node, up to 5 replicas, asynchronous replication, multi-az enabled by default for failover. horizontal scale out/in by adding/removing read replicas. vertical scale up/down the node type. elasticache will internally create a new node group and replicate data to the new group.
     - cluster mode enabled: data is partitioned across shards(helpful to scale write), each shard is the same as `cluster mode disabled`, multi-az capability. up to 500 nodes per cluster(consists of a bunch of shards)
       - auto scaling: increase/decrease the desired shards or replicas. support both target tracking and scheduled scaling policies, only for `cluster mode enabled`. the cloudwatch metric: `elasticachePrimaryEngineCPUUtilization`, and update apps to use the cluster `configuration endpoint`
       - redis connection endpoints: `standalone node`: one endpoint for read and write. `cluster mode disabled cluster`: primary endpoint for all write, reader endpoint for evenly split read across all read replicas, node endpoint for read. `cluster mode enabled cluster`: configuration endpoint for all read/write, node endpoint for read
 - dynamodb
   - overview: full-managed, highly available with replication across multiple az. no maintenance or patching, always available. standard & infrequent access(IA) table class. `basics`: made of tables, each table has a primary key and has an infinite numer of items. each item size is 400kb at maximal. support scalar types, document types, set types. can repidly evolve schemas. `read/write capacity modes`: provisioned mode(default) with auto-scaling feature. on-demand mode: auto scale up/down, more expensive, good for unpredictable workloads
   - advanced features
     - accelerator: fully-managed, highly available, seamless in-memory cache for dynamodb, help solve read congestion, microseconds latency for cached data, 5 min TTL default, no app logic modification. for elasticache, it's good for storing aggregation result.
     - stream processing: ordered stream of item-level modifications in a table. use cases: react to real-time changes, data analytics, cross-region replication... `dynamodb streams`(24 hrs, limited consumers) and `kinesis data streams`(1 yr retention, more consumers,)
     - global tables: two-way replications, active-active replication(read/write in any region), must enable dynamodb stream first
     - TTL: delete items after an expiry timestamp. use cases: reduce stored data by only keeping current items, adhere to regulatory obligations, web session handling...
     - backups for disaster recovery:
       - continuous backups using point-in-time recovery(PITR): optionally enabled for last 35 days, or recover to any time within the backup window, it will crete a new table.
       - on-demand backups: full backups for long-term retention until deleted explicitly, no affect on performance or latency, can be config and managed by aws backup, it will create a new table
     - integration with s3: export to s3(enable PITR)--> works for last 35 days, no affect on read capacity, can perform data analysis, or ETL on top of s3 data, format: json or ion. import from s3--> csv, dynamodb json or ion format, no consume on write capacity, create a new table, any error will be sent to cloudwatch logs
 - AWS DMS
   - overview: quickly and securely migrate databases to aws, resilient, self healing, no affect on source database. support homogeneous migrations and heterogeneous migrations. continuous data replication using CDC(change data capture). an ec2 instance must be running DMS.
     - sources: on-prem dbs, azure sql db, aws rds including aurora, s3, documentdb
     - targets: on-prem dbs, aws rds, redshift, opensearch , kinesis data stream, kafka, nepture, documentdb,redis
     - aws schema conversion tool(sct): convert db schema from one engine to another
     - multi-az deployment: standby replica(synchronously)
   - monitoring:
     - replication task monitoring: task status(task status bar), table state
     - cloudwatch metrics: host metrics, replication task metrics, table metrics 
 - S3 - replication: cross-region replication(crr, compliance, replication across accounts, low latency access), same-region replication(srr, log aggregation, live replication between prod and test accounts). copying is asynchronous, must have iam permissions
 - AWS storage gateway
   - overview: hybrid cloud, unlike EFS/NFS, s3 is a proprietary storage technology, to expose s3 data on-prem, we need aws storage gateway(allow on-prem to use aws cloud s3 data). use cases: disaster recovery, backup&restore, tiered storage. types: file, volume, tape
   - file gateway cache refresh: `RefreshCache api`--> file gateway will automatically update the file cache when user write files to file gateway which will be synced with s3 bucket. or call `RefreshCache` api to refresh the cache for file gateway if a user upload file directly to s3. `Automating cache refresh`--> automatically refresh file gateway periodically
 - Auto scaling groups
   - scaling policies:
     - dynamic scaling: `target tracking scaling`,`simple/step scaling`(with cloudwatch)
     - scheduled scaling: for known usage patterns
     - predictive scaling: continuously forecast load and schedule scaling ahead
     - good metrics to scale on: `CPUUtilization`, `RequestCountPerTarget`, `Average Network In/Out`, `Any custom metric`
     - scaling cooldown: cooldown period(default 300 sec) after a scaling activity, during which asg will allow for metrics to stablize. advice: use ready-to-use ami to reduce configuration time in order to be serving request fasters and reduce the cooldown period.
   - lifecycle hooks: you can perform some actions before an ec2 instance is launched or terminated, can be integrated with eventbridge, sns, sqs
   - event notifications: sns notifications: 4 events for asg: ec2_instance_launch, ec2_instance_launch_error, ec2_instance_terminate, ec2_instance_terminate_error. eventbridge: can have more events on different conditions(successful, failed,cancelled...) 
   - termination policies: `default termination policy`: select az with more instances, terminate one with the oldest launch template or launch configuration, or terminate one with the same launch template that is closest to the next billing hour. `aoolcationStrategy`, `OldestLaunchTemplate`,`OldestLaunchConfiguration`,`ClosestToNextInstanceHour`,`NewestInstance`,`OldestInstance`. **note**: can use one or more policies, just define the evaluation order. also can define custom termination policy backed by lambda function
   - warm pools: scale-out latency strategy by maintaining a pool of pre-initialized instances(running--not cost saving, stopped, hibernated). warm pool not contribute to asg metrics that affect scaling policies. warm pool size: `minimum warm pool size`,`max prepared capacity`,`or max prepared capacity`
     - instance reuse policy: by default, asg will terminate instance when scale in, then launch a new instance in warm pool. while instance reuse policy allow to return instances to the warm pool when scale in
     - warm pool--lifecycle hooks(warmed:pending:wait event)
 - application auto scaling: monitor your apps and automatically adjusts capacity to maintain steady, predictable performance at lowest cost. setup scaling for multiple resources across multiple services from a single place. point to your app and select the service and resource you want to scale(no need alarms and scaling actions for each service). search for resources/services using cloudformation stack,tags,or ec2 asg. build `scaling plans` to automatically add/remove capacity from your resources in real-time as demand changes. support target tracking , step, and scheduled scaling policies.
 - ELB
   - ALB rules deep dive: each rule has a target, support actions(forward,redirect,fixed-reponse), conditions: host-header,request method,path pattern,source IP, http header, query string. `target group weighting`: specify weight for each target group, example:blue/green deployment. distribute the traffic for your apps.
   - extra: `dualStack networking`: allow clients talk with elb using ipv4 and ipv6, support ALB and NLB, can have mixed ipv4 and ipv6 targets in seperate target groups. **note**: az must be added/enabled for instances to receive traffic. `privatelink integration`: for overlapping ip addresses in two vpcs, instead of using vpc peering, we create vpc interface endpoint on one end(vpc1), and connect to NLB at the other end(vpc2)
 - NAT gateway: aws-managed NAT, az-specific, cannot be used by ec2 instances in the same subnet, require an internet gateway, no security group required
   - high availability: resilient in a single az, must create multiple NAT gateways in multiple az.
 - multi-AZ architectures: implicit(s3 except onezone-IA, dynamodb, all aws managed services), explicit(EFS,ELB,ASG,beanstalk,Rds,elasticache,aurora,opensearch,jenkins)
 - blue-green architectures: for an ALB, define two target groups(blue,green), and connect them to the same listener, then switch traffic at once or use weighted TG. or if we have one ALB for one target group, then we can use route53 switch traffic, but that depends on clients ttl cache. if using api gateway, we can use `prod stage canary`, or more granular, we can use lambda alias, no changes to api gateway or client
 - multi-region architectures: with route53: health check-->automated dns failover. monitor an endpoint, monitor other health checks, or monitor cloudwatch alarms. health checks are integrated with cloudwatch metrics.
 - disaster recovery:
   - types: `on-prem -> on-prem`(traditional but expensive), `on-prem -> cloud`(hybrid), `cloud region 1 -> cloud region 2`(fast, inexpensive). `RPO`(recovery point objective),`RTO`(recovery time objective)
   - strategies:
     - backup and restore(High RPO,RTO)
     - pilot light(a small version of app is always running, useful for the critical core, such as db)
     - warm standby(full system is up and running, but at minimum size)
     - hot site approach(very low RTO, very expensive, full production scale is running on aws and on-prem, route53-active active)
   - tips:
     - backup: ebs snapshot, rds automated backup/snapshot,... regular push to s3 and other storage classes. from on-prem using snowball or storage gateway.
     - high availability: route53 to direct from region to region, rds, elasticache, efs multi-az, s3. site-to-site vpn as a recovery from direct connect
     - replication: rds replication, aurora + global db, replication from on-prem db to rds, storage gateway
     - automation: cloudformation, elastic beanstalk to rebuild new env, recover ec2 if cloudwatch alarm triggered, lambda for customized automation.
     - chaos: ex. netflix randomly terminateing ec2 instances
## Monitoring and Logging
 - cloudwatch metrics: metric is a variable to monitor, which belongs to namespaces, a dimension is an attribute of a metric. up to 30 dimensions per metric, metric has timestamps. cloudwatch dashboard of metrics, can create custom metrics.
   - metric streams: near real-time delivery, targets: firehose, or 3rd party services. or option to filter metrics to only stream a subset of them. 
 - cloudwatch custom metrics: use api `PutMetricData` to send your own custom metrics to cloudwatch, can also use dimensions. metric resolution `storageResolution` api parameter: standard: 1 min, hight resolution: 1/5/10/30 sec high cost. **important**: accepts metric data points two weeks in the past and two hours in the future(make sure ec2 instance configured correctly) 
 - cloudwatch anomaly detection: continuously analyze metrics to determine normal baselines and surface anomalies using ML algorithm. a model will be created based on the past data, and show you the values out of the normal range, allow you to create alarms based on metric's expected value(instead of static threshold), can also exclude specified time periods or events from being trained.
 - amazon lookout for metrics: more complete, automatically detect anomalies and identify root cases using ML without manual intervention. can integrate with different aws services (not just cloudwatch)and 3rd party saas apps through appFlow
 - cloudwatch-logs: `log groups`,`log stream`,`log expiration policies`, by default encrypted, or can use kms, logs can be sent to s3, kinesis data streams, kinesis data firehose, lambda, opensearch
   - source: SDK, cloudwatch unified agent, cloudwatch log agent, ecs, lambda, vpc flow logs, api gateway, cloudtrail, route53 dns log
   - insights: cloudwatch logs insights: search and analyze log data, built-in query language, can query multiple log groups in different aws accounts, not real-time
   - s3 export: logs data needs up to 12 hrs to become available for export, api call `CreateExportTask`, not real-time, use logs subscriptions instead.
   - logs subscriptions: get real-time log events with subscription filter, then send logs to kinesis data streams, firehose or lambda.
   - cloudwatch log aggregation multi-account & multi-region
   - cross-account subscription
 - cloudwatch-logs - live tail
 - cloudwath-logs - metric filters: cloudwatch logs can use filter expression, the filter dont retroactively filter data, only publish the metric data points for events that happen after the filter was created. able to specify up to 3 dimensions for the metric filter(optional)
 - all kinds of logs:
   - application logs: produced by app code, written to a local file system. on ec2, use cloudwatch agent to send logs. for lambda, ecs or fargate, or elastic beanstalk, direct integration with cloudwatch logs
   - operating system logs(event logs, system logs): produced by ec2 or on-prem, informing system behavior, using cloudwatch agent to send to cloudwatch logs
   - access logs: list of all the requests for individual files that people have requested from website. usually load balancers, proxies, web servers,etc. aws provides some access logs
   - aws managed logs: ELB access logs-->s3, cloudtrail logs--> s3 and cloudwatch logs, vpc flow logs --> s3 and cloudwatch logs, route53 access logs --> cloudwatch logs, s3 access logs --> s3, cloudfront access logs --> s3
 - cloudwatch agent & cloudwatch logs agent: install cloudwatch logs agent on ec2 or on-prem to send logs to cloudwatch logs. `cloudwatch logs agent` & `unified agent`(newer version, can collect additional system-level metrics such as ram, processes,etc), use ssm parameter store to config centrally
   - unified agent -- metrics: CPU, disk metrics, ram, netstat, processes, swap space
 - cloudwatch alarms: various options(%,min,max...). state: ok, insuffient_data, alarm. period: length of time to evaluate the metric.
   - targets: actions on ec2 instances. ec2 auto scaling. amazon sns
   - composite alarms: monitor multiple states of alarms, using `and` and `or` conditions, help to reduce `alarm noise`
   - ec2 instance recovery: status check(instance status, system status). recovery: same private, public, elastic Ip, metadata, placement group
   - good to know: can create alarms based on metrics filters. to test alarms and notifications, set the alarm state to `Alarm` using cli
 - cloudwatch synthetics canary: a configurable script that monitor your apis, URLs, websites... reproduce what your customers do to find issues before customes are impacted. check the availability and latency of your endpoints and can store the load time data and screenshots of the UI. written in nodejs or python. can access to a chrome browser, can run once or on a regular schedule.
   - blueprint: `heartbeat monitor`,`api canary`,`broken link checker`,`visual monitoring`,`canary recorder`,`gui workflow builder`  
 - amazon Athena: serverless query service to analyze data in s3 using standard SQL language, support csv,json,orc,arvo, and parquet. commonly used with aws quicksight for dashboard. use case: BI/analytics/analyze& query vpc flow logs, elb logs, cloudtrail logs, etc....
   - performance improvement: use `columnar data` for cost-savings(less scan), apache parquet or orc is recommended. use `aws Glue` to transform data to parquet or orc. `compress data` for smaller retrieves. `partition datasets` in s3 for easy querying(only query certain amount of data). `use larger files`(> 128mb) to minimize overhead
   - federated query: can run sql query across sql or non-sql dbs or custom data sources(aws, on-prem). use `data source connectors` that run on aws lambda to run federated queries(such as cloudwatch logs, dynamodb, rds,...), then store the results back in s3.
## Incident and Event Response
 - eventBridge
   - overview: schedule--cron jobs; event pattern--event rules to react to a service doing something; trigger lambda functions, send sqs/sns messages... can optionally filter events. can have default event bus, partner event bus, and custom event bus. a reource-based policies can be applied to event buses. can archive events(all/filter for indefinitely or a period), able to replay archived events for debugging. can infer the event schema, and schema registry allows you to generate code for your app, schema can be versioned. eventbridge uses resource-based policy to manage access. use case: aggregate all events from aws organization in a single aws account or region.
   - content filtering
   - input transformation
 - S3 - event notifications: deliver events in secs or mins. for sns, sqs, lambda, we define resource policy for access management. all s3 events will be sent to eventbridge which has more than 18 destinations of aws services: advanced filtering, multiple destinations, eventbridge capabilities
 - S3 - object integrity: s3 uses `checksum` to validate the integrity of uploaded objects. `using MD5`: user calculated MD5 and add a http header: content-md5 to s3, and s3 will calculate md5 itself and then compare the two. `using md5 & etag`: s3 has an etag for object which is equal to md5 if using sse-s3, then `get object metadata`, compare the etag with local version.
 - AWS health dashboard
   - overview: `service history`(show all regions, all services health, historical info for each day, has a RSS feed to subscribe to), `your account`(provides alerts and remediation guidance when aws is facing events that may impact you, giving personal view into performance and availability of aws services you are using. shows relevant and timely info to help manage events in progress and provides proactive notif to help plan for scheduled activities. can aggregate data from entire aws organization. global service)
   - events & notifications: integrate with eventbridge to react to aws health events in your account(possible for account events--affected resources in your account and public events--regional availability of a service). use cases: send notifs, take corrective action, capture event info, remediation...
 - EC2 instance status checks: `system status checks`(any software/hardware issues on the physical host, check aws health dashboard for any critical maintenance. resolution: stop and start a new instance--instance migrated to a new host), `instance status checks`(software/network config, resolution: reboot the instance or change instance config)
   - status checks -- cloudwatch metrics & recovery:
     - metrics: `statusCheckFailed_system`,`statusCheckFailed_instance`,`statusCheckFailed`(for both)
     - option1: cloudwatch alarm-- recover ec2 instance with the same private/public IP, EIP, metadata, and placement group.
     - option2: asg-- set `min/max/desired` to 1 to recover an instance which will launch a new instance(ips will be different)
 - cloudtrail
   - overview: get a history of events/api calls in your aws account. can put logs from cloudtrail to cloudwatch logs or s3. a trail can be applied to single region or all regions(default).(if a resource is deleted, investigate the cloudtrail first)
     - management events(default): operations happened on resources, can separate `read events` and `write events`
     - data events: by default data events are not logged(high volume operations)
     - cloudtrail insights events(not free): has to enable to detect unusual activity in your account. analyze normal management events to create a baseline, then continuously analyzes write events to detect unusual patterns, the insights events appear in the cloudtrail console, sent to s3 and eventbridge
     - cloudtrail events retention: 90 days in cloudtrail, then log them to s3 and use athena to analyze. 
   - eventBridge integration: cloudtrail events will appear in the eventbridge
 - SQS - dead letter queues: we can set `MaximumReceives` threshold, then message will go to dead letter queue if exceeded. dlq type should be the same with the source queue(fifo--fifo,standard--standard). make sure to process messages in the dlq befire expiry date(good to set retention). `redrive to source`: manual inspection and debugging, once fixed, redrive the message from dlq to the source queue in batches without writing custom code.
 - SNS - redrive policy: for messages not delivered successfully, we set a dlq(sqs, or sqs fifo), and create a `redrive policy`. the dlq is attached to sns subscription-level(not topic level)
 - AWS X-Ray: visual analysis of our apps(distributed system). tracing requests across your microservices. integration with ec2(x-ray agent),ecs(x-ray agent or docker container),lambda, beanstalk(automated agent installed),api gateway(helpful to debug, like 504). need iam permissions to x-ray. 
 - AWS X-Ray with Beanstalk: beanstalk include x-ray daemon, just to enable `x-ray`. make sure instance profile has iam permissions for x-ray daemon to function correctly. make sure app code has x-ray sdk. **note**: x-ray daemon not provided for multi-container
 - AWS Distro for openTelemetry: vendor-agnostic, open-source, production-ready aws-supported distributed tracing framework. collect distributed traces and metrics from your apps, and collect metadata from your aws resources and services. `auto-instrumentation agents` to collect traces without changing your code. can send to x-ray, cloudwatch, prometheus... migrate from x-ray to openTelemetry if for standard apis or send traces to multiple destinatioins simultaneously.
## Security and Compliance
 - AWS config
   - overview: help audit and record compliance of your aws resources. can receive alerts for any changes. per-region service. can be aggregated across regions and accounts. possible to storing config data in s3
     - rules: managed rules or custom rules. can be evaluated or triggered for any change or at regualr intervals. rules dont prevent actions from happening. no free tier.
     - resource: view compliance, config, cloudtrail api calls of resources over time.
     - remediations: can trigger ssm automation document to remediate non_compliant issues.
     - notifications: use eventbridge to trigger notifs when non_compliant. use sns to send notif when config or conpliance changes
   - configurations recorder and aggregator: `recorder`: store configurations of aws resources as `configuration item` which is a point-in-time view of all attributes of an aws resource. created whenever a change detected. the recorder only record the resource types you specify. must created before aws config can track(when using aws cli or console to enable aws config, created automatically). `aggregator`: created in one central aggregator account. aggregator rules, resources, etc... across multiple accounts and regions. if using aws organization, no need for individual authorization. rules are created in each individual source aws account. can deploy rules using cloudformation stacksets to multiple targets
   - conformance packs: collection of aws config rules and remediation actions. in yaml format, similar to cloudformation. deploy to an aws account and regions or across an aws organization. pre-built sample or custom packs. can include `custom config rules` backed by lambda to evaluate  the compliance. can use `ssm parameter store`. can designate a delegated admin to deploy conformance packs to your aws organization(a member account). can be integrated into cicd
   - organizational rules: conformance rules-->accounts and organization. organizational rules-->only for aws organization, and managed at organizational level, one rule at a time vs many rule at a time(conformance packs)
 - AWS organizations
   - overview: `OrganizationAccountAccessRole` has full permissions in the member account to the management account, used to perform admin tasks in the member account(e.g. create iam users), can be assumed by iam users in the management account. automatically added to all new member accounts created in the aws organization, but must created this role manually if to invite an existing member account.
     - multi account strategies: create accounts based on envs, or departments, or regulatory restrictions(using SCP) / multi account vs one account multi vpc / using tagging standards for billing purposes / enable cloudtrail on all accounts to send logs to central s3 account / send cloudwatch logs to central logging account / create an account for security.
     - feature modes: `consolidated billing features`: single payment method for all accounts, pricing benefits from aggregated usage(volume discount for ec2, s3...). `all eatures`(default): include consolidated billing, scp. invited accounts must approve enabling all features, able to apply scp to prevent member accounts from leaving the org. cannot switch back to `consolidated billing feature only`
     - reserved instances: `consolidated billing feature` treat all accounts as one account, all accounts can benefit from reserved instances purchased by any other account, the payer account(management account) can turn off the sharing of reserved instance discount and saving plans discount. and to share the reserved instance or saving plan discount with an account, both accounts must have sharing turned on.
     - moving account between orgs: first remove member account, then invite it from another org, then accept the invitation.  
   - service control policy (SCP): allow or deny iam actions. applied to OU or Account level. does not apply to management account. scp is applied to all users or roles in the account, including root user. does not affect service-linked roles which used for other aws service to talk with aws org. scp must have explicit `allow` to allow some actions
 - AWS control tower
   - overview: runs on top of aws organization. setup and govern a secure and compliant multi-account aws env based on best practice. automate setup envs, policy management using guardrails, detect policy violations and remediate them, monitor compliance through a dashboard.
     - account factory: automate account provisioning and deployments using aws service catalog, enables you to create pre-approved baseline and config options for aws accounts in your org.
     - detect and remediate policy violations: `guardrail`: preventive(using scp), detective(using aws config)
     - guardrails levels: `mandatory`(enforced by aws control tower), `strongly recommended`(based on aws best practice ,optional),`elective`(commonly used by enterprise, optional)
   - landing zones: automatically provision secure, compliant, multi-account env based on aws best practices. it consists of: aws organization, account factory, organizational units, service control policies(scp), iam identity center, guardrails(rules and policies), aws config(monitor and assess resources compliance with guardrails)
   - account factory & migrating accounts
     - account factory customization(AFC): automatically customize resources in new and existing accounts created through account factory. `custom blueprint`: cloudformation template with resources and configs used to customize accounts. defined in the form of service catalog product. recommend to create a `hub account` to store all custom blueprints. only one blueprint can be deployed to the account. each time a new account created, an event will be sent to eventbridge.
     - migrate an aws account to control tower: first move the account to the unregistered OU, then create iam role(awsControlTowerExecution), then create conformance packs, then evaluate compliance results of config conformance packs, then remove config delivery & config recorder created during evaluation process, then move into target OU and setup control tower successfully.
   - customizations for AWS control tower (CFCT): gitops-style customization framework created by aws, helps add customization to your landing zone using custom cloudformation templates and scps. automatically deploy resources to new aws accounts created using account factory.**note**: cfct is different from afc(account factory customization), blueprint
   - aws config integration: using aws config to implement `detective guardrails`, automatically enabled aws config in enabled regions. aws config configuration history and snapshots sent to s3 bucket in a centralized log archive account. control tower uses cloudformation stacksets to create resources like config aggregator,...
     - `aws config conformance packs`: a set of config compliance rules & remediations. for example, when a new account created in control tower, an event will be sent to eventbridge which can be used to trigger a lambda to create stacksets and deploy conformance packs into the account.
   - account factory for terraform(AFT): help provision and customize aws accounts in control tower through terraform using a deployment pipeline. create `account request terraform file` to trigger aft workflow for account provisioning. `built-in feature options`(disabled by default): `aws cloudtrail data events`,`aws enterprise support plan`,`delete the aws default vpc`. terraform module maintained by aws. works with terraform open-source, terraform enterprise, and terraform cloud.
 - IAM identity center
   - overview: successor to aws single sign-on: one login for all aws accounts in aws organization. identity providers: built-in identity store in iam identity center, 3rd party: active directory, okta...
     - permission sets assigned to users and groups
     - similar to other apps, such as salesforce, slack, micro 365, or some custom apps
     - attribute-based access control(ABAC): fine-grained permissions based on users' attributes. define permission once, then change the value of attributes of users.
   - extra: `external identity providers`: saml2.0, you must create users and groups in iam identity center that are identical to the users and groups in the external identity providers, because saml2.0 cannot query the idP to learn about users and groups. thus we can use `SCIM`(system for cross-domain identity management): automatic provisioning(synchronization) of users identities from an external idp into iam identity center. must be supported by external idp, and perfect complement to using smal2.0
   - attribute-based access control: fine-grained permissions based on users' attributes stored in iam identity center identity store. user attributes can be used in permission sets and resource-based policy. user attributes are mapped from idp as key-value pairs
   - multi-factor authentication: every time they sign-in(always on), or only when their sign-in context changes(context-aware)
 - AWS web application firewall (WAF): protect web apps from common web exploits (layer 7). on ALB(localized rules), on Api gateway(rules running at the regional or edge level), on cloudfront(rules globally on the edge locations), on Appsync(protect your graphql apis). not for ddos protection(using aws shield). define web ACL(access control list): include ip addresses, http headers, body, or url strings, protect common attack--sql injection, xss. size constraints, geo match, rate-based rule, rule actions: count | allow|block|captcha
   - managed rules: over 190 managed rule, ready-to-use rules. `baseline rule groups`: general protection from common threats, `use-case specific rule groups`,`ip reputation rule groups`,`bot control managed rule group`
   - web ACL logging: can send logs to cloudwatch logs (5mb per sec), s3 bucket(5 min interval), kinesis data firehose(limited by firehose quotas)
   - solution architecture -- enhance cloudfront origin security with aws waf & aws secrets manager: we can setup aws waf at cloudfront with web acl(some protection), then from cloudfront, we can create `custom HTTP header`(such as x-origin-verify:xxxxxxxx), then setup aws waf in front of our ALB to filter the http header. the http header string can be managed by secret manager with auto-rotate and a lambda used to update the custom http header string.
 - AWS firewall manager
   - overview: manage rules in all accounts of aws organization. rules are applied to new resources as they are created across all accounts or future account in the organization.
     - security policy: `waf rules`, `aws shield advanced`,`security groups`, `aws network firewall(vpc level)`, `route53 resolver DNS firewall`, policies are created at the region level.
     - aws waf, firewall manager, shield
   - policies
     - aws waf: enforce web acls to all albs in all account of aws organization. identify resources that dont comply, but do not auto remediate them. or auto remediate any non-compliant resources
     - shield advanced: enforece shield advanced protections to all accounts in the aws org. option to view only compliance or auto remediate
     - security groups: common(applying sgs to all ec2 instances in all accounts in the aws org). auditing(check and manage sgs rules in all accounts in the aws org). usage audit(monitor unused and redundant sgs and optionally perform cleanup)
     - network firewall(all kinds of traffic, layer3): centrally manage network firewall in all accounts in aws org. `distributed`(create and manage firewall endpoint in each vpc), `centralized`(create and manage firewall endpoint in a centralized vpc), `import existing firewalls`(using `Resource sets`)
     - route53 resolver dns firewall: manage associations between resolver dns firewall rule groups and vpcs in all accounts in aws org
 - Amazon guardduty
   - overview: threats discovery service using ML(30 days trial). input data: vpc flow logs, dns logs, cloudtrail events logs. can setup eventbridge to get notified. can protect against `cryptoCurrency` attacks (has a dedicated finding for it)
   - advanced:
     - multi-account strategy: admin account send invitation through guardduty to member accounts. and admin account can add/remove member account, manage guardduty within the associated member accounts, manage findings, suppression rules, trusted ip lists, threat lists. also you can specify a member account as a delegated admin for guardduty.
     - findings automated response: potential security issues in the aws account, any findings will be sent to eventbridge as events, from which we can send to sqs, sns, lambda. events are published to the admin account and the member account that its orginated from
     - findings: guardduty pulls independent streams of data from cloudtrail logs, vpc flow logs, dns logs or eks logs. each finding has a severity value (0.1 -- 8+)(high, medium, low). can generate sample findings in guardduty to test your automations. naming convention: threatPurpose, resourceTypeAffected, threatFamilyName, detectionMechanism(TCP,udp),Artifact
     - findings types: ec2 finding types, iam finding types, kubernetes audit logs finding types, malware protection finding types, rds protection finding types, s3 finding types
     - architectures
     - trusted and threat ip lists: only for public ip address, only guardduty admin account can manage those lists(trusted IP lists and threat IP lists)
   - cloudformation integration: can enable guardduty using cloudformation template. but it will fail if guardduty already enabled. to solve it, use cloudformation custom resource(lambda) to conditionally enable guardduty if it is not enabled, and we can deploy this stack set to all organization.
 - amazon detective: analyze and investigate and identify the root cause of security issues or suspicious activities using ML and graphs. automatically collects and process events from vpc flow logs, cloudtrail, and guardDuty and create a unified view. produce visualizations with details and context to get to the root cause.
 - amazon inspector
   - overview: security accessments. for ec2 instance, using ssm agent to analyze network accessibility, os known vulnerabilities. for container images push to aws ecr, scan images, for lambda function, scan software vulnerabilities, code, packages. then send reports to aws security hub and send findings to eventbridge. a risk score is associated with all vulnerabilities for priorirization. continuous scanning infra only when needed.
   - EC2 setup: having ssm agent running in the ec2 instance with iam role or default host management config for ssm, outbound 443 to ssm endpoint. then aws inspector is evaluating the inventory results in the ssm.
 - EC2 instance migration using AMIs: create an AMI, then launch/restore a new ec2 instance in another az from the AMI.
   - cross-account AMI sharing: sharing AMI does not affect the ownership of the AMI. can only share AMIs that have unencrypted volumes and encrypted volumes with cmk. if you share AMI with encrypted volumes, you must also share cmk to the other side and iam permissions.
   - cross-account AMI copy: to copy AMI shared with you, you are the owner of the target AMI in your account. the owner must grant the read permission for the storage backing the AMI(ebs snapshot). if shared AMI has encrypted snapshots, then the owner must share the key with you as well. can encrypt AMI with your own cmk while copying.
 - AWS trusted advisor
   - overview: no need to install anything--high level aws account assessment. 6 categories: cost optimization, performance, security(default),fault tolerance,service limits(default),operational excellence. upgrade to `business` or `enterprise` support plan to unlock all checks. 
   - architectures: integrated with eventbridge to monitor some resources such as usage of ec2 instance, or some service quotas (50+ service quotas)
 - AWS secrets manager: newer service, meant for storing secrets. able to force rotation of secrets every x days. automate generation of secrets on rotation(lambda). secrets encrypted using kms, integrated with rds, aurora. mostly meant for rds integration.
   - multi-region secrets: read replicas synced with primary secret. use cases: disaster recovery, multi-region apps, multi-region db...
## Other Services
 - AWS tag editor: allow to manage tags of multiple resources at once. add/update/delete. search tagged/untagged resources in all aws regions
 - AWS quicksight: serverless ML BI service to create interactive dashboards. fast, automatically scalable, embeddable, with per-session pricing. use cases: data visualization, business analytics, ad-hoc analysis, get business insight. integrated with rds, aurora, athena, redshift, s3...
 - AWS glue: managed ETL service, fully serverless service.
   - convert data into parquet format(columnar type, which is good for athena).
   - glue data catalog: catalog of datasets-- glue data crawler to write metadata of data sources into glue data catalog tables(metadata), then athena, redshift spectrum, emr will leverage it
   - things to know:
     - `glue job bookmarks`(prevent re-processing old data).
     - `glue elastic views`: combine and replicate data across multiple data stores using SQL, no custom code, glue monitor for changes in the source data
     - `glue databrew`: clean and normalize data using pre-built transformation
     - `glue studio`: new gui to create, run and monitor etl jobs in glue
     - `glue streaming etl`(built on apache spark structured streaming): compatible with kinesis data streaming, kafka, MSK(aws managed kafka)

## Filling the gap
 - ### udemy assessment:
   - for aws cloudtrail, it can monitor api activity and generate alerts based on suspicious or unauthorized activity. But it may not provide sufficient visibility into the application and infrastructure performance.
   - While X-Ray can be used to troubleshoot distributed application, it may not provide sufficient visibility into the infrastructure and can be limited to specific application types. It must be paired with CloudWatch to provide visibility into the application and infrastructure, and enable timely detection and resolution of potential issues.
   - While CPU utilization monitoring can provide insights into the health of instances, it does not actively perform health checks or ensure high availability and fault tolerance.
   - aws elasticache cluster serves as a data cache layer, not a data layer.
   - Set up an Amazon EventBridge rule to trigger a series of AWS Lambda functions orchestrated by AWS Step Functions, which is a highly efficient and scalable approach to handle high-volume event streams. because step function is automatic scaling.
   - Using AWS Systems Manager Automation documents enables the automation of remediation actions based on the noncompliant resource findings from AWS Config rules.
   - Set up IAM Identity Providers to establish trust relationships with external identity providers, which enables users to log in using their existing credentials, facilitating centralized access control across multiple AWS accounts and external applications.
   - Configure Amazon Kinesis Data Streams to capture and process incoming events, with AWS Lambda processing functions consuming the events for distributed event-driven processing. This approach provides scalability, fault tolerance, and efficient distribution of events to multiple downstream services. Amazon Kinesis Data Streams is purpose-built for handling high volumes of streaming data and enables real-time processing and analysis.
   - User data scripts provide a way to automate the configuration process during instance initialization. This allows for consistent installation and configuration of agents as instances are launched.
   - AWS Systems Manager Distributor enables controlled deployment of software packages, including agents, across multiple instances, allowing for efficient installation and configuration.
   - Use AWS OpsWorks to automatically install and configure agents on the EC2 instances based on a predefined configuration. This allows for centralized management and automation of the agent installation process.
   - API Gateway is a good option to create API's for event triggers, Lambda is used for processing as it takes care of infrastructure management, and Fargate ia best suited for container orchestration.
   - when it comes to fail to deploy lambda function due to a permission error, we can check cloudwatch logs for the lambda as well as iam role for the lambda function. but please note that the lambda function resource policies is for accessibility not for deployment purpose.
   - Cookbooks can modify the configuration and the state of any system configured as a node on Chef infrastructure and so can be used to automate the configuration and deployment of applications and software on servers. It is primarily used for configuration management. CodePipeline is a continuous delivery service that can be used to model, visualize, and automate deployment and so can be used to automate the build, test, and deployment of code changes.
   - Systems Manager's State Manager allows you to define and automate the configuration of your EC2 instances, ensuring that they are all configured according to your organization's security standards. **note**: AWS Config provides a detailed inventory of your AWS resources and can track configuration changes over time. However, it does not provide direct support for configuring instances to meet compliance requirements.
   - Use AWS CloudTrail logs for comprehensive visibility of your AWS environment activities. Implement Amazon GuardDuty for continuous threat detection and use AWS Lambda functions to trigger real-time responses to identified threats.
   - Use CloudWatch Logs for centralized log collection and set up alarms on custom metrics, along with AWS X-Ray for analyzing application traces for deeper insights
   - Use a combination of latency, request count, and CPU utilization metrics for dynamic scaling, which provides an accurate representation of the application's performance, enabling dynamic scaling based on traffic patterns. This approach can efficiently handle traffic spikes and ensure consistent application performance.
   - To deploy an application using the AWS Elastic Beanstalk environment, the application code should be uploaded first and the configuration options specified. Create an Elastic Beanstalk environment, upload the application code, and then specify the configuration options
   - The design phase typically involves the creation of user stories, wireframes, and high-level architecture designs. During this phase, the requirements gathered during the analysis phase are used to create the design of the application.

     
 - ### bonus questions
   - Create a replication cluster managed by EC2 with Auto Scaling in eu-west-1. Scale according to a Custom Metric you would publish with the application representing the lag in file reads. Replicate the data into Amazon S3 in ap-southeast-2. Create another replication cluster in ap-southeast-2 that reads from Amazon S3 and copies the files into a standby EFS cluster. we need to create a custom metric via the application that captures the lag in file reads and then uses it for scaling the ASG managing the EC2 instances to replicate the source EFS cluster into S3. Use another ASG to copy data from S3 into EFS in the target AWS Region. Here we want minimum RPO so we want continuous replication, and minimum RTO so we want a hot EFS system ready to go. Please note that because the RPO and RTO are low, the cost of the solution will be very high.
   - Create a separate Beanstalk environment that's a worker environment and processes invoices through an SQS queue. The invoices are uploaded into S3 and a reference to it is sent to the SQS by the web tier. The worker tier processes these files. A cron job defined using the cron.yml file will send out the emails.
   - If you enable DynamoDB Streams on a table, you can associate the stream Amazon Resource Name (ARN) with a Lambda function that you write. Immediately after an item in the table is modified, a new record appears in the table's stream. AWS Lambda polls the stream and invokes your Lambda function synchronously when it detects new stream records. No more than two processes at most should be reading from the same streams shard at the same time. Having more than two readers per shard can result in throttling. Therefore, you need to use a fan-out pattern for this, SNS being perfect for that.
   - If an Amazon EC2 Auto Scaling scale-up event occurs while a deployment is underway, the new instances will be updated with the application revision that was most recently deployed, not the application revision that is currently being deployed. If the deployment succeeds, the old instances and the newly scaled-up instances will be hosting different application revisions. To resolve this problem after it occurs, you can redeploy the newer application revision to the affected deployment groups. To avoid this problem, AWS recommends suspending the Amazon EC2 Auto Scaling scale-up processes while deployments are taking place. You can do this through a setting in the common_functions.sh script that is used for load balancing with CodeDeploy. If HANDLE_PROCS=true, the following Amazon EC2.
   - Create a CodePipeline pointing to the master branch of your CodeCommit repository and automatically deploy to a staging environment using CodeDeploy. After that stage, invoke a CodeBuild build that will run the test suite. If the stage doesn't fail, the last stage will deploy the application to production.
   - Create a log destination in the centralized account, and create a log subscription on that destination. Create a Kinesis Firehose delivery stream and subscribe it to the log destination. The target of Kinesis Firehose should be Amazon S3.
   - Run the application in an Auto Scaling Group and scale based on the CloudWatch Metric `MillisBehindLatest`. The retention period is the length of time that data records are accessible after they are added to the stream. A stream’s retention period is set to a default of 24 hours after creation. To avoid records being dropped, it's good to increase the stream retention time and allow ourselves a higher margin to process the records. The maximum retention you can set is 7 days.
   - You can use AWS Systems Manager to create a maintenance window, and then register an Automation task to automate the creation of the AMIs. This process is applicable for both Windows and Linux instances.
   - The AMI ID is region-scoped, so the AMI must be copied across regions and therefore each SSM parameter store will have different AMI ID values. But you can still use the same SSM Parameter Store key across all regions.
   - You can use CloudFormation to deploy and update compute, database, and many other resources in a simple, declarative style that abstracts away the complexity of specific resource APIs. CloudFormation is designed to allow resource lifecycles to be managed repeatably, predictably, and safely, while allowing for automatic rollbacks, automated state management, and management of resources across accounts and regions. Here, the issue is that CloudFormation does not detect a new file has been uploaded to S3 unless one of these parameters change: - S3Bucket - S3Key - S3ObjectVersion.
   - In the `ValidateService` hook in appspec.yml, verify the service is properly running. Configure CodeDeploy to rollback on deployment failures. In case the hook fails, then CodeDeploy will rollback. For the given use-case, you can use `ValidateService` hook to verify that the deployment was completed successfully. This is the last deployment lifecycle event. You can configure CodeDeploy to rollback if this hook fails.
 
 
 - ### practice exam #1
   - you can install patches on a regular basis by scheduling patching to run as a Systems Manager maintenance window task. Systems Manager supports an SSM document for Patch Manager, AWS-RunPatchBaseline, which performs patching operations on instances for both security-related and other types of updates. When the document is run, it uses the patch baseline currently specified as the "default" for an operating system type. The AWS-ApplyPatchBaseline SSM document supports patching on Windows instances only and doesn't support Linux instances. For applying patch baselines to both Windows Server and Linux instances, the recommended SSM document is AWS-RunPatchBaseline.
   - When you perform some operations using the AWS Management Console, Amazon S3 uses a multipart upload if the object is greater than 16 MB in size. In this case, the checksum is not a direct checksum of the full object, but rather a calculation based on the checksum values of each individual part. For example, consider an object 100 MB in size that you uploaded as a single-part direct upload using the REST API. The checksum in this case is a checksum of the entire object. If you later use the console to rename that object, copy it, change the storage class, or edit the metadata, Amazon S3 uses the multipart upload functionality to update the object. As a result, Amazon S3 creates a new checksum value for the object that is calculated based on the checksum values of the individual parts.
   - Amazon EventBridge events for AWS Glue can be used to create Amazon SNS alerts, but the alerts might not be specific enough for certain situations. To receive SNS notifications for certain AWS Glue Events, such as an AWS Glue job failing on retry, you can use AWS Lambda. You can create a Lambda function to do the following: 1)Check the incoming event for a specific string. 2)Publish a message to Amazon SNS if the string in the event matches the string in the Lambda function.
   - The AWS Config Auto Remediation feature automatically remediates non-compliant resources evaluated by AWS Config rules. You can associate remediation actions with AWS Config rules and choose to execute them automatically to address non-compliant resources without manual intervention. You must have AWS Config enabled in your AWS account. The AutomationAssumeRole in the remediation action parameters should be assumable by SSM. The user must have pass-role permissions for that role when they create the remediation action in AWS Config, and that role must have whatever permissions the SSM document requires.
   - cloudwatch agent cannot deliver ec2 instance logs to s3. cloudwatch agent and cloudtrail cannot deliver ec2 logs and api logs from ec2 instances to kinesis data stream.
   - One way to verify the integrity of your object after uploading is to provide an MD5 digest of the object when you upload it. If you calculate the MD5 digest for your object, you can provide the digest with the PUT command by using the Content-MD5 header. After uploading the object, Amazon S3 calculates the MD5 digest of the object and compares it to the value that you provided. Or The entity tag (ETag) for an object represents a specific version of that object. Keep in mind that the ETag reflects changes only to the content of an object, not to its metadata. If only the metadata of an object changes, the ETag remains the same. For objects where the ETag is the Content-MD5 digest of the object, you can compare the ETag value of the object with a calculated or previously stored Content-MD5 digest.
   - CloudFormation StackSets simplify the configuration of cross-account permissions and allow for the automatic creation and deletion of resources when accounts are joined or removed from your Organization. You can get started by enabling data sharing between CloudFormation and Organizations from the StackSets console. Once done, you will be able to use StackSets in the AWS Organizations master account to deploy stacks to all accounts in your organization or specific organizational units (OUs). A new service-managed permission model is available with these StackSets. Choosing `Service managed permissions` allows StackSets to automatically configure the necessary IAM permissions required to deploy your stack to the accounts in your organization. In addition to setting permissions, CloudFormation StackSets offer the option for automatically creating or removing your CloudFormation stacks when a new AWS account joins or quits your Organization. You do not need to remember to manually connect to the new account to deploy your common infrastructure or to delete infrastructure when an account is removed from your Organization. When an account leaves the organization, the stack will be removed from the management of StackSets. However, you can choose to either delete or retain the resources managed by the stack.
   - With Amazon CloudWatch cross-account observability, you can monitor and troubleshoot applications that span multiple accounts within a Region. Seamlessly search, visualize, and analyze your metrics, logs, and traces in any of the linked accounts without account boundaries. The shared observability data can include the following types of telemetry: 1. Metrics in Amazon CloudWatch 2. Log groups in Amazon CloudWatch Logs 3. Traces in AWS X-Ray. AWS recommends that you use Organizations so that new AWS accounts created later in the organization are automatically onboarded to cross-account observability as source accounts. This is our use case requirement and hence choosing Organizations to implement the requirements.
   - This option suggests using EC2 Image Builder to create an updated custom AMI with AWS Systems Manager Agent included. The Auto Scaling group is configured to attach the `AmazonSSMManagedInstanceCore` role to the instances, thereby enabling centralized management through Systems Manager, as it grants the EC2 instances the permissions needed for core Systems Manager functionality. Session Manager can be used for secure logins, and session details can be logged to Amazon S3. Additionally, an S3 event notification can be set up to alert the security team about new file uploads using Amazon SNS. This option aligns well with the requirement for centralized access and monitoring.
   - In some cases, a Blue/Green deployment fails during the AllowTraffic lifecycle event, but the deployment logs do not indicate the cause for the failure. This failure is typically due to incorrectly configured health checks in Elastic Load Balancing for the Classic Load Balancer, Application Load Balancer, or Network Load Balancer used to manage traffic for the deployment group. To resolve the issue, review and correct any errors in the health check configuration for the load balancer.
   - Configure Amazon Route 53 to point to API Gateway APIs in Europe and Asia-Pacific regions. Use latency-based routing and health checks in Route 53. Configure the APIs to forward requests to an AWS Lambda function in the respective Regions. Setup the Lambda function to retrieve and update the data in a DynamoDB global table.
   - when you create a role, you create two policies: a trusted policy and a permission policy. to assume a role from a different account, your aws account must be trusted by the role. the trust relationship is defined in the role's trust policy when the role was created. so in this case, Establish a trust relationship in the application account's deployment IAM role for the centralized DevOps account, allowing the sts:AssumeRole action. Also, grant the application account's deployment IAM role the necessary access to the EKS cluster. Additionally, configure the EKS cluster aws-auth ConfigMap to map the role to the appropriate system permissions.
   - If you create a customer-managed key in a different account than the Auto Scaling group, you must use a grant in combination with the key policy to allow cross-account access to the key. This is a two-step process (refer to the image attached): 1.The first policy allows Account A to give an IAM user or role in the specified Account B permission to create a grant for the key. However, this does not by itself give any users access to the key. 2.Then, from Account B which contains the Auto Scaling group, create a grant that delegates the relevant permissions to the appropriate service-linked role. The Grantee Principal element of the grant is the ARN of the appropriate service-linked role. The key-id is the ARN of the key.
   - Launch a CloudFormation stack that deploys all the resources of the stack. Add a Retain attribute to the deletion policy of each of these resources. Delete the original stack. Create a new stack with a different name and import the resources that were retained from the original stack. Remove the Retain attribute from the stack to revert to the original template
   - Attribute-based access control (ABAC) is an authorization strategy that defines permissions based on attributes. In AWS, these attributes are called tags. You can attach tags to IAM resources, including IAM entities (users or roles), and to AWS resources. You can create a single ABAC policy or a small set of policies for your IAM principals.
   - Use AWS CodeDeploy with a deployment type configured to Blue/Green deployment configuration. To terminate the original fleet after two hours, change the deployment settings of the Blue/Green deployment. Set Original instances value to Terminate the original instances in the deployment group and choose a waiting period of two hours. In AWS CodeDeploy Blue/Green deployment type, for deployment groups that contain more than one instance, the overall deployment succeeds if the application revision is deployed to all of the instances. The exception to this rule is that if deployment to the last instance fails, the overall deployment still succeeds. This is because CodeDeploy allows only one instance at a time to be taken offline with the CodeDeployDefault.OneAtATime configuration (If you don't specify a deployment configuration, CodeDeploy uses the CodeDeployDefault.OneAtATime deployment configuration). If you choose Terminate the original instances in the deployment group: After traffic is rerouted to the replacement environment, the instances that were deregistered from the load balancer are terminated following the wait period you specify.
   - AWS WAF doesn't support encryption for AWS Key Management Service keys that are managed by AWS. You can enable logging AWS WAF web ACL traffic, to get detailed information about traffic that is analyzed by your web ACL. Logged information includes the time that AWS WAF received a web request from your AWS resource, detailed information about the request, and details about the rules that the request matched. You can send your logs to an Amazon CloudWatch Logs log group, an Amazon Simple Storage Service (Amazon S3) bucket, or an Amazon Kinesis Data Firehose. Your bucket names for AWS WAF logging must start with aws-waf-logs- and can end with any suffix you want.
   - Use Serverless Application Model (SAM) and leverage the built-in traffic-shifting feature of SAM to deploy the new Lambda version via CodeDeploy and use pre-traffic and post-traffic test functions to verify code. Rollback in case CloudWatch alarms are triggered.
   - A stack set lets you create stacks in AWS accounts across regions by using a single CloudFormation template. All the resources included in each stack are defined by the stack set's CloudFormation template. As you create the stack set, you specify the template to use, in addition to any parameters and capabilities that the template requires. always remember to use stacksets to deploy your infra to multiple regions.
   - Add a reader instance to the Aurora cluster. Update the application configuration to use the Aurora cluster endpoint for write operations. Update the Aurora cluster reader endpoint for read operations. aurora replicas serve two purposes, one is to help offload read requests from the main DB(increasing reading scalability), second is to help increase availability. The cluster endpoint provides failover support for read/write connections to the DB cluster. If the current primary DB instance of a DB cluster fails, Aurora automatically fails over to a new primary DB instance. During a failover, the DB cluster continues to serve connection requests to the cluster endpoint from the new primary DB instance, with minimal interruption of service.
   - With the pilot light approach, you replicate your data from one Region to another and provision a copy of your core workload infrastructure. Resources required to support data replication and backup, such as databases and object storage, are always on. Other elements, such as application servers, are loaded with application code and configurations, but are "switched off" and are only used during testing or when disaster recovery failover is invoked. In the cloud, you have the flexibility to de-provision resources when you do not need them, and provision them when you do. A best practice for “switched off” is to not deploy the resource, and then create the configuration and capabilities to deploy it (“switch on”) when needed. Unlike the backup and restore approach, your core infrastructure is always available and you always have the option to quickly provision a full-scale production environment by switching on and scaling out your application servers.
   - One way to execute code and actions before terminating an instance is to create a lifecycle hook that puts the instance in `Terminating:Wait` status. This allows you to perform any desired actions before immediately terminating the instance within the Auto Scaling group. The `Terminating:Wait` status can be monitored by an Amazon CloudWatch event, which triggers an AWS Systems Manager automation document to perform the action you want. Broadly, the steps needed for the above configuration: 1. Add a lifecycle hook. 2. Create a Systems Manager automation document. 3. Create AWS Identity and Access Management (IAM) policies and a role to delegate permissions to the Systems Manager automation document. 4. Create IAM policies and a role to delegate permissions to CloudWatch Events, which invokes the Systems Manager automation document. 5. Create a CloudWatch Events rule. 6. Add a Systems Manager automation document as a CloudWatch Event target.
   - Configure the AWS Lambda function to use provisioned concurrency. Configure the application Auto Scaling on the Lambda function with provisioned concurrency values of 1 (minimum) and 100 (maximum) respectively. this will alleviate cold start issues but cost more accordingly.
   - AWS Firewall Manager offers the freedom to use multiple AWS accounts and to host applications in any desired region while maintaining centralized control over their organization’s security settings and profile. Firewall Manager is built around named policies that contain WAF rule sets and optional AWS Shield advanced protection. Each policy applies to a specific set of AWS resources, specified by account, resource type, resource identifier, or tag. Policies can be applied automatically to all matching resources, or to a subset that you select. there are 3 prerequisites: aws organization; firewall administrator--you must designate one of the aws accounts in your organization as admin; aws config--must enable aws config so that firewall manager can detect newly created resources. (aws config can track the status of resources, but firewall manager can centrally manage the security infra)
   - Create a target tracking auto-scaling policy that targets an average CPU utilization of 50 percent for an application that runs on Spot Fleet. You can create and manage target tracking using the AWS CLI, SDKs, or CloudFormation. Use Scheduled scaling Auto Scaling policy, and create a scheduled action with AWS Lambda function as a scalable target. Specify the minimum and maximum capacity based on the requirements. AWS CLI, SDKs, or CloudFormation can be used for configuring the schedule scaling.
   - Create an AWS KMS key to use with CodePipeline in the development account. Also, the input bucket from the development account must have versioning activated to work with CodePipeline. You must use the AWS Key Management Service (AWS KMS) customer-managed key for cross-account deployments. If the key isn't configured, then CodePipeline encrypts the objects with default encryption, which can't be decrypted by the role in the destination account. Configure a cross-account role in the production account. Attach a policy to your CodePipeline service role in the development account that allows it to assume the cross-account role that you created. In the deploy action, the CodePipeline service role assumes the cross-account role in the production account. CodePipeline uses the cross-account role to access the KMS key and artifact bucket in the development account. Then, CodePipeline deploys the extracted files to the production output S3 bucket in the production account.
   - Set up the Amazon CloudWatch Agent on each Amazon EC2 instance with the configuration to push all logs to Amazon CloudWatch Logs. Create a CloudWatch metric filter to detect user logins. Use Amazon SNS to notify the security team when a login is detected. The Amazon CloudWatch agent can be installed on EC2 instances to collect and send log data to CloudWatch Logs. By setting up a metric filter within CloudWatch Logs, it is possible to search for specific patterns, such as user login events. If a user login is found in the log data, Amazon SNS is used to send an immediate notification to the security team. You can use CloudWatch Logs to monitor applications and systems using log data in near real-time. **note**: While AWS CloudTrail captures login events, it typically delivers logs within an average of about 5 minutes of an API call. However, this time is not guaranteed. Therefore, this option is incorrect.
   - Set up AWS Config in the AWS account. Use a managed rule for Resource Type EC2::Volume that returns a compliance failure if the custom tag is not applied to the EBS volume. Configure a remediation action that uses custom AWS Systems Manager Automation documents (runbooks) to apply the custom tag with predefined backup frequency to all non-compliant EBS volumes.
   - Configure the AWS Lambda function to send the response(SUCCESS or FAILED) of the custom resource creation to a pre-signed Amazon Simple Storage Service URL. Custom resources enable you to write custom provisioning logic in templates that AWS CloudFormation runs anytime you create, update (if you changed the custom resource), or delete stacks. Use the `AWS::CloudFormation::CustomResource` or `Custom::MyCustomResourceTypeName` resource type to define custom resources in your templates. Custom resources require one property: the service token, which specifies where AWS CloudFormation sends requests to, such as an Amazon SNS topic. The custom resource provider processes the AWS CloudFormation request and returns a response of SUCCESS or FAILED to the pre-signed URL. The custom resource provider responds with a JSON-formatted file and uploads it to the pre-signed S3 URL. If this URL is not provided, the calling template will not get an update of the status of the Lambda function and will remain in an in-progress state.
   - CloudTrail is an AWS service that helps you enable governance, compliance, and operational and risk auditing of your AWS account. Actions taken by a user, role, or AWS service are recorded as events in CloudTrail. An event in CloudTrail is the record of activity in an AWS account. This activity can be an action taken by a user, role, or service that is monitorable by CloudTrail. CloudTrail events provide a history of both API and non-API account activity made through the AWS Management Console, AWS SDKs, command line tools, and other AWS services. CloudTrail data events are disabled by default. You can enable logging at an additional cost. Data events are also known as data plane operations and are often high-volume activities. Data events aren't viewable in CloudTrail event history and are charged for all copies at a reduced rate compared to management events. CloudTrail records management events for the last 90 days free of charge, and are viewable in the Event History with the CloudTrail console. For Amazon S3 delivery of CloudTrail events, the first copy delivered is free. Additional copies of management events are charged.
   - AWS CloudFormation supports resource import and drift detection operations for only supported resource types. Custom resource types are not currently supported. AWS Config rule depends on the availability of `DetectStackDrift` action of CloudFormation API. AWS Config defaults the rule to `NON_COMPLIANT` when throttling occurs. AWS Config rule depends on the availability of DetectStackDrift. You receive a throttling or "Rate Exceeded" error because AWS Config defaults the rule to NON_COMPLIANT when throttling occurs.
   - When rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones, so that rebalancing does not compromise the performance or availability of your application. Because Amazon EC2 Auto Scaling attempts to launch new instances before terminating the old ones, being at or near the specified maximum capacity could impede or completely stop rebalancing activities. To avoid this problem, the system can temporarily exceed the specified maximum capacity of a group by a 10 percent margin (or by a margin of one instance, whichever is greater) during a rebalancing activity. When an Auto Scaling group with a mixed instances policy scales in, Amazon EC2 Auto Scaling still uses termination policies to prioritize which instances to terminate, but first it identifies which of the two types (Spot or On-Demand) should be terminated. It then applies the termination policies in each Availability Zone individually. Amazon EC2 Auto Scaling always balances instances across Availability Zones first, regardless of which termination policy is used. As a result, you might encounter situations in which some newer instances are terminated before older instances.
   - Execution role and user permissions: If the file system doesn't have a user-configured AWS Identity and Access Management (IAM) policy, EFS uses a default policy that grants full access to any client that can connect to the file system using a file system mount target. If the file system has a user-configured IAM policy, your function's execution role must have the correct elasticfilesystem permissions. Update the Lambda execution roles with permission to access the VPC and the EFS file system. Create a VPC peering connection to connect Account A to Account B. Update the EFS file system policy to provide Account B with access to mount and write to the EFS file system in Account A. Configuring a file system and access point: To connect an EFS file system with a Lambda function, you use an EFS access point, an application-specific entry point into an EFS file system that includes the operating system user and group to use when accessing the file system, file system permissions, and can limit access to a specific path in the file system. This helps keep file system configuration decoupled from the application code.
   - You should note that no permissions are granted by an SCP.
   - AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. By creating a CodePipeline action with an AWS Lambda function immediately after the API deployment stage, the DevOps team can automate the process of downloading the SDK from API Gateway and uploading it to the S3 bucket. Additionally, the Lambda function can create a CloudFront invalidation for the SDK path, ensuring that web clients get the latest SDK without any caching issues. You cannot use any S3 API to invalidate the CloudFront cache.
   - `BeforeAllowTraffic` hook is used to run tasks before traffic is shifted to the deployed Lambda function version. By using the "BeforeAllowTraffic" hook, the DevOps team can ensure that traffic to the new version of the Lambda function is allowed only after necessary database changes have fully propagated, thus avoiding intermittent failures after deployment.
   - Server access logging provides detailed records for the requests that are made to an Amazon S3 bucket. Server access logs are useful for many applications. For example, access log information can be useful in security and access audits. It can also help you learn about your customer base and understand your Amazon S3 bill.
   - Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, It is not a vulnerability management service.
   - Amazon Inspector is an automated vulnerability management service that continually scans Amazon Elastic Compute Cloud (EC2), AWS Lambda functions, and container workloads for software vulnerabilities and unintended network exposure. To successfully scan Amazon EC2 instances for software vulnerabilities, Amazon Inspector requires that these instances have the SSM agent installed. Configure CloudTrail with CloudWatch Logs to monitor your trail logs and be notified when specific activity occurs. You can use Amazon CloudWatch Logs to monitor, store, and access your log files from Amazon Elastic Compute Cloud (Amazon EC2) instances, AWS CloudTrail, Route 53, and other sources.
   - You can use AWS Config to evaluate the configuration settings of your AWS resources. You do this by creating AWS Config rules, which represent your ideal configuration settings. AWS Config provides customizable, predefined rules called managed rules to help you get started. While AWS Config continuously tracks the configuration changes that occur among your resources, it checks whether these changes do not comply with the conditions in your rules. You can then create an Amazon EventBridge rule (with AWS Config configured as a source) that is put in action when it matches the NON_COMPLIANT evaluation result of the restricted-ssh rule. The EventBridge rule, in turn, publishes a notification to the SNS topic. (Set up an Amazon EventBridge rule that matches an AWS Config evaluation result of NON_COMPLIANT for the restricted-ssh rule. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic)
   - A Blue/Green deployment is a deployment strategy in which you create two separate, but identical environments. Using a Blue/Green deployment strategy increases application availability and reduces deployment risk by simplifying the rollback process if a deployment fails. Once testing has been completed on the green environment, live application traffic is directed to the green environment and the blue environment is deprecated. Several AWS deployment services support Blue/Green deployment strategies including Elastic Beanstalk, OpsWorks, CloudFormation, CodeDeploy, and Amazon ECS. the blue group carries the production load while the green group is staged and deployed with the new code. When it’s time to deploy, you simply attach the green group to the existing load balancer to introduce traffic to the new environment. As you scale up the green Auto Scaling group, you can take the blue Auto Scaling group instances out of service by either terminating them or putting them in a Standby state.
   - to deploy a cloudformation stack from account A to account B using aws codepipeline: 1)In account A, create a customer-managed AWS KMS key that grants usage permissions to account A's CodePipeline service role and account B. Also, create an Amazon Simple Storage Service (Amazon S3) bucket with a bucket policy that grants account B access to the bucket. 2)In account B, create a cross-account IAM role. In account A, add the AssumeRole permission to account A's CodePipeline service role to allow it to assume the cross-account role in account B. 3)In account B, create a service role for the CloudFormation stack that includes the required permissions for the services deployed by the stack. In account A, update the CodePipeline configuration to include the resources associated with account B.
   - When an update to a CloudFormation stack fails, AWS CloudFormation automatically initiates a rollback process to revert the stack to its previous known stable state. In certain cases, such as when there are dependencies on external resources, the rollback process might stall or encounter an error. To help recover from a failed stack update, you can use the ContinueUpdateRollback command. This command instructs CloudFormation to continue the stack rollback and can help resolve the UPDATE_ROLLBACK_FAILED state. A dependent resource can't return to its original state, causing the rollback to fail (UPDATE_ROLLBACK_FAILED state). For example, you might have a stack that's rolling back to an old database instance that was deleted outside of AWS CloudFormation. Because AWS CloudFormation doesn't know the database was deleted, it assumes that the database instance still exists and attempts to roll back to it, causing the update rollback to fail. Depending on the cause of the failure, you can manually fix the error and continue the rollback. By continuing the rollback, you can return your stack to a working state (the UPDATE_ROLLBACK_COMPLETE state), and then try to update the stack again.
   - Create an AWS CloudFormation service role with required permissions and associate this service role to the stack. Grant the developer iam:PassRole permissions to pass the role to the service. Use this newly created service role during stack deployments. A service role is an AWS Identity and Access Management (IAM) role that allows AWS CloudFormation to make calls to resources in a stack on your behalf. Use a service role to explicitly specify the actions that AWS CloudFormation can perform, which might not always be the same actions that you or other users can do. When you specify a service role, AWS CloudFormation always uses that role for all operations that are performed on that stack. It is not possible to remove a service role attached to a stack after the stack is created. Other users that have permission to perform operations on this stack will be able to use this role, but they must have the `iam:PassRole` permission. To pass a role (and its permissions) to an AWS service, a user must have permission to pass the role to the service. This helps administrators ensure that only approved users can configure a service with a role that grants permissions. To allow a user to pass a role to an AWS service, you must grant PassRole permission to the user's IAM user, role, or group.
   - Create a CloudWatch Logs subscription to deliver the login event data of Amazon EC2 instances to an AWS Lambda function. Configure the Lambda function to add a decommission tag to the EC2 instance that produced the login event. Schedule an Amazon EventBridge rule to invoke another Lambda function every hour, to terminate all EC2 instances with the decommission tag. **note**: Amazon CloudWatch alarm does not support having AWS Lambda function as an alarm action.
   - Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding and configure a CloudWatch alarm on this custom metric to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic. **note**: The term "metric stream" specifically refers to a feature within CloudWatch Metrics that allows you to continuously stream metric data to a destination of your choice. This feature is separate from CloudWatch Logs and serves a different purpose. This feature allows for near-real-time delivery and low latency of metrics data. While AWS destinations like Amazon S3 are supported for metric streams, Kinesis Data Firehose is not supported as a destination for metric streams.


 - ### practice exam #2
   - `Define commonly used architectures as CloudFormation templates. Create Service Catalog stacks from these templates, and ensure the tagging is done properly. Place the IAM users into a beginner group and allow the users to only launch stacks from Service Catalog, while restricting any write access to other services`. AWS Service Catalog allows IT administrators to create, manage, and distribute catalogs of approved products to end-users, who can then access the products they need in a personalized portal. Administrators can control which users have access to each product to enforce compliance with organizational business policies. A product is a service or application for end-users. A portfolio is a collection of products, with configuration information that determines who can use those products and how they can use them. A catalog is a collection of products that the administrator creates, adds to portfolios, and provides updates for using AWS Service Catalog. To create a Service Catalog product, you first need to create an AWS CloudFormation template by using an existing AWS CloudFormation template or creating a custom template. Then you can use the AWS Service Catalog console to upload the template and create the product.
   - `Create an AWS Config rule to track if CloudTrail is enabled. Create a CloudWatch Event rule to get alerted in case of breaches, and trigger a Lambda function that will re-enable CloudTrail`. You need to have an AWS Config rule to maintain auditability and track compliance over time. You can use the `cloudtrail-enabled` Config managed rule to check whether AWS CloudTrail is enabled in your AWS account. You can use `cloudtrail-security-trail-enabled` Config managed rules to check that there is at least one AWS CloudTrail trail defined with security best practices. To be alerted of compliance issues, use a CloudWatch Event rule and then hook it to a Lambda function that will re-enable CloudTrail automatically.
   - `Use AWS Config to track resources in your account. Use SNS to stream changes to a Lambda function that writes to S3. Create a QuickSight dashboard on top of it`. Here, we can use AWS Config to track resource configuration, and we could create a rule to track the tagging of these resources. All the changes to resource configuration as well as tagging of resources are streamed to an SNS topic.
   - `Run the application in an Auto Scaling Group and scale based on the CloudWatch Metric MillisBehindLatest`. The Kinesis Client Library (KCL) ensures that for every shard, there is a record processor running and processing that shard. The library also simplifies reading data from the stream. The Kinesis Client Library uses an Amazon DynamoDB table to store control data. For the given use-case, you need to run KCL on multiple EC2 instances behind an ASG. Running more KCL processes is the key here, and we need for that to have an Auto Scaling Group based on the metric MillisBehindLatest, which represents the time that the current iterator is behind from the latest record (tip) in the shard. The Kinesis Client Library (KCL) for Amazon Kinesis Data Streams publishes custom Amazon CloudWatch metrics on your behalf, using the name of your KCL application as the namespace. `Increase the stream data retention period`. The retention period is the length of time that data records are accessible after they are added to the stream. A stream’s retention period is set to a default of 24 hours after creation. To avoid records being dropped, it's good to increase the stream retention time and allow ourselves a higher margin to process the records. The maximum retention you can set is 7 days.
   - `Manage the EC2 instances using OpsWorks. Include a chef cookbook on the configure lifecycle event that will update the configuration file accordingly`. OpsWorks lets you use Chef and Puppet to automate how servers are configured, deployed, and managed across your Amazon EC2 instances or on-premises compute environments. A stack is the top-level AWS OpsWorks Stacks entity. It represents a set of instances that you want to manage collectively, typically because they have a common purpose such as serving PHP applications. In addition to serving as a container, a stack handles tasks that apply to the group of instances as a whole, such as managing applications and cookbooks. Every stack contains one or more layers, each of which represents a stack component, such as a load balancer or a set of application servers. Each layer has a set of five lifecycle events, each of which has an associated set of recipes that are specific to the layer. When an event occurs on a layer's instance, AWS OpsWorks Stacks automatically runs the appropriate set of recipes. The lifecycle hook that is called on ALL instances, whenever an instance comes up or another one goes down, is the `configure` hook. So this option is the best fit for the given use-case.
   - **note**: lambda function ssh into ec2 instances is not a practicable, it won't work.
   - `At the end of the pipeline in eu-west-1, include an S3 step to copy the artifacts being used by CodeDeploy to an S3 bucket in us-east-2. Make the CodePipeline in us-east-2 source files from S3`. For the given use-case, you can use an S3 deploy step to copy artifacts into another bucket. Then CodePipeline in the other region will respond to an event and source the files from the other bucket and kickstart the deployment pipeline there.
   - **note**: s3 can also be included in a pipeline as a deploy action
   - `Enable CloudTrail. Create a CloudWatch Event rule to track an AWS API call via CloudTrail and use SNS as a target`. For the given use-case, we can use the 'AWS API Call via CloudTrail' feature of CloudWatch Events and set up SNS as a target to achieve the desired outcome.
   - **note**: DynamoDB Streams do not capture `DeleteTable` API calls, they only capture item-level events.
   - `Create a CodePipeline that will invoke a CodeBuild stage. The CodeBuild stage should acquire ECR credentials using the CLI helpers, build the image, and then push it to ECR. Upon the success of that CodeBuild stage, start a CodeDeploy stage with a target being your ECS service`. You can use CodeBuild to acquire ECR credentials using the CLI helpers, build the image, and then push it to ECR. You should note that acquiring ECR credentials **must be done using IAM roles and CLI helpers on CodeBuild, not environment variables, especially not via your user access and secret key.**
   - **note**: codeDeploy: ec2/on-prem; lambda; ecs tasks
   - `Create a CloudFormation template with an ASG of min/max capacity of 1, and an EBS volume. Tag the ASG and EBS volume. Create a User Data script that will acquire the EBS volume at boot time. Use a master CloudFormation template and reference the nested template 6 times`. For the given use-case, you need to leverage CloudFormation to set up 6 ASGs of 1 instance each and EBS volumes with the appropriate tags and then use an EC2 user data script to attach the corresponding EBS volumes correctly.
   - **note**: ebs is AZ scoped.
   - `Create a replication cluster managed by EC2 with Auto Scaling in eu-west-1. Scale according to a Custom Metric you would publish with the application representing the lag in file reads. Replicate the data into Amazon S3 in ap-southeast-2. Create another replication cluster in ap-southeast-2 that reads from Amazon S3 and copies the files into a standby EFS cluster`.
   - **note**: Side note (for your knowledge) the AWS DataSync service (not covered in the exam) can achieve EFS to EFS replication in a much more native way.
   - **note**: With this solution, as the files are copied to S3, the file Linux permissions would not be replicated.
   - `Create a CloudWatch metrics stream and direct it to Kinesis Firehose Firehose delivery stream. Send all the metrics data into S3 using Firehose, and create a QuickSight dashboard to visualize the metrics. Use Athena to query for specific time ranges`. You can create a metric stream and direct it to an Amazon Kinesis Data Firehose delivery stream that delivers your CloudWatch metrics to a data lake such as Amazon S3. other supported destinations include AWS destinations such as Amazon Simple Storage Service and several third-party service provider destinations. The Kinesis Data Firehose delivery stream must trust CloudWatch through an IAM role that has write permissions to Kinesis Data Firehose.
   - `Turn on API calls logging using AWS CloudTrail. Deliver the logs in an S3 bucket, and use the log verification integrity API call to verify the log file`.  To determine whether a log file was modified, deleted, or unchanged after CloudTrail delivered it, you can use `CloudTrail log file integrity validation`. This feature is built using industry-standard algorithms: SHA-256 for hashing and SHA-256 with RSA for digital signing. For the given use-case, to track API calls made within your account, you need to use AWS CloudTrail. Then the right way to verify log integrity would be to use the CloudTrail `validate-logs` command.
   - **Create a CloudWatch Event rule for `aws.opsworks` and set the `initiated_by` field to `auto-healing`. Target a Lambda function that will send notifications out to the Slack channel**. A stack is the top-level AWS OpsWorks Stacks entity. It represents a set of instances that you want to manage collectively, typically because they have a common purpose such as serving PHP applications. In addition to serving as a container, a stack handles tasks that apply to the group of instances as a whole, such as managing applications and cookbooks. Every stack contains one or more layers, each of which represents a stack component, such as a load balancer or a set of application servers. Each layer has a set of five lifecycle events, each of which has an associated set of recipes that are specific to the layer. When an event occurs on a layer's instance, AWS OpsWorks Stacks automatically runs the appropriate set of recipes. OpsWorks Stacks provides an integrated management experience that spans the entire application lifecycle including resource provisioning, EBS volume setup, configuration management, application deployment, monitoring, and access control. You can send state changes in OpsWorks Stacks, such as instance stopped or deployment failed, to CloudWatch Events. The `initiated_by` field is only populated when the instance is in the requested, terminating, or stopping states. The initiated_by field can contain one of the following values.
   - **note**: CloudWatch Events does not have a direct integration with Slack
   - `Create a CodePipeline pointing to the master branch of your CodeCommit repository and automatically deploy to a staging environment using CodeDeploy. After that stage, invoke a CodeBuild build that will run the test suite. If the stage doesn't fail, the last stage will deploy the application to production`.
   - `Two new instances were created during the deployment`. If an Amazon EC2 Auto Scaling scale-up event occurs while a deployment is underway, the new instances will be updated with the application revision that was most recently deployed, not the application revision that is currently being deployed. If the deployment succeeds, the old instances and the newly scaled-up instances will be hosting different application revisions. To resolve this problem after it occurs, you can redeploy the newer application revision to the affected deployment groups. To avoid this problem, AWS recommends suspending the Amazon EC2 Auto Scaling scale-up processes while deployments are taking place. You can do this through a setting in the common_functions.sh script that is used for load balancing with CodeDeploy. If HANDLE_PROCS=true, the following Amazon EC2 Auto Scaling events are suspended automatically during the deployment process: `azrebalance`, `alarmnotification`, `scheduledactions`,`replaceunhealth`,`replaceunhealthy`.
   - `Create a new Lambda function version and release it. Create a new API Gateway Stage and deploy it to the v2 stage. Both use the same Lambda function as a backing route for the v1 and v2 stages. Add a static mapping on the v1 route to add "color": "none" on requests`. For the given use-case, API Gateway mappings must be used. API Gateway lets you use mapping templates to map the payload from a method request to the corresponding integration request and from an integration response to the corresponding method response. As such, you must deploy a v2 API alongside the v1 API backed by the same Lambda function. Old clients will hit the v1 API, which will use a mapping template to add the static missing field "color": "none". Newer clients will hit the v2 API and will have that field value included.
   - `Create a CloudWatch Event with a daily schedule, the target being a Step Function. The Step Function should launch an EC2 instance from the AMI and tag it with CheckVulnerabilities: True. The Step Function then starts an AMI assessment template using AWS Inspector and the above tag. Terminate the instance afterward`. So to summarize, the most cost-effective and the least disruptive way to do an assessment is to create an EC2 instance from an AMI for that very purpose, run the assessment and then finally terminate the instance. Step Functions are perfect to orchestrate that workflow by targeting the instances tagged with CheckVulnerabilities: True.
   - `Create a CloudFormation template to enable CloudTrail. Create a StackSet and deploy that StackSet in all your accounts and regions under the AWS organization. Create another CloudFormation StackSet to enable AWS Config, and create a Config rule to track if CloudTrail is enabled. Create an AWS Config aggregator for a centralized account to track compliance. Create a CloudWatch Event to generate events when compliance is breached, and subscribe a Lambda function to it, that will send out notifications`. For the given use-case, we need to enable CloudTrail and AWS Config in all accounts and all regions. For this, we'll need separate StackSets to create CloudTrail and enable Config in all accounts and all regions. Note that we'll also need an AWS Config aggregator in a centralized account. Finally, compliance breaches would generate CloudWatch events that can be subscribed by a Lambda function to further send out notifications.
   - `Create a CloudWatch Event Rule that reacts to the creation and updates done to Pull Requests in the source repository. The target of that rule should be CodeBuild. Create a second CloudWatch Event rule to watch for CodeBuild build success or failure event and as a target invoke a Lambda function that will update the pull request with the Build outcome`. For the given use-case, we need to create two CloudWatch Event Rules. The first rule would trigger on CodeCommit Pull Request and have the target as CodeBuild. The second rule would trigger on CodeBuild build success or failure event and have the target as a Lambda function that will update the pull request with the Build outcome.
   - `Create a CloudWatch Logs Metric Filter and assign a CloudWatch Metric. Create a CloudWatch Alarm linked to the metric and use SNS as a target. Create an email subscription on SNS`. For the given use-case, you can have Beanstalk send the logs to CloudWatch Logs, and then create a metric filter. This will create a metric for us (and not an alarm), and on top of the metric, you can create a CloudWatch Alarm. This alarm will send a notification to SNS, which will, in turn, send us emails.
   - `Create a CloudWatch Event rule on your CodeCommit repository that reacts to pushes. As a target, choose an AWS Lambda function that will request the code from CodeCommit, zip it and send it to the 3rd party API`. Amazon CloudWatch Events delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources. Using simple rules that you can quickly set up, you can match events and route them to one or more target functions or streams. You can generate custom application-level events and publish them to CloudWatch Events. You can also set up scheduled events that are generated on a periodic basis. A rule matches incoming events and routes them to targets for processing.
   - `Create an SSM Automation document to create the AMI in a repeatable manner`. You can use AWS Systems Manager to create a maintenance window, and then register an Automation task to automate the creation of the AMIs. This process is applicable for both Windows and Linux instances. `Store the AMI ID in the SSM parameter store in one region, and have a Lambda function that copies the AMI across all the other regions, and stores the corresponding AMI ID in SSM. Use the same parameter store name so it can be re-used across regions`. The AMI ID is region-scoped, so the AMI must be copied across regions and therefore each SSM parameter store will have different AMI ID values. But you can still use the same SSM Parameter Store key across all regions.
   - Create a `buildspec.yml` file that will look for the environment variable `CODEBUILD_SOURCE_VERSION` at runtime. Use the variable in the `artifacts` section of your `buildspec.yml` file. For the given use-case, we need to use environment variables. The variable `CODEBUILD_SOURCE_VERSION` is exposed at runtime directly within CodeBuild and represents the branch name of the code being tested for CodeCommit. This is the best solution.
   - Create a file named `.ebextensions/alb.config` in your code repository and add an `option_settings` block for which you will specify the Rules for the key `aws:elbv2:listener:default`. Push your code and let the CodePipeline run. You can use Elastic Beanstalk configuration files (.ebextensions) with your web application's source code to configure your environment and customize the AWS resources that it contains. Configuration files are YAML- or JSON-formatted documents with a .config file extension that you place in a folder named .ebextensions and deploy in your application source bundle. You can use the `option_settings` key to modify the environment configuration. You can choose from general options for all environments and platform-specific options.
   - `Using SSM, implement a Custom Patch Baseline. Define a Maintenance window and include the Run Command RunPatchBaseline. Schedule the maintenance window with a weekly recurrence`.
   - `Enhance the health check so that the return status code corresponds to the connectivity to the database`. You could just add a simple health check endpoint to the ALB which accepts a request and immediately responds with an HTTP status of 200. This approach provides for a fast health check, but would not meet the requirement for the given use-case. You need to improve the quality of the health check and make sure it returns a proper status code. As the application depends on the database, you need to ensure that you include health checks for these components when determining the health of your service. (to add some logic in that endpoint to make sure it will return proper response which can be used for health check)
   - `Create a CodeCommit repository and create a CodePipeline pipeline that will deploy any changes to the master branch to the development and staging environment. Create a second CodePipeline pipeline that will deploy changes to the production branch to the production environment after the code is merged through a pull request`. Here you only need one git repository and create a production branch for deploys to production. The other key requirement of the given use-case is that two versions of the code need to be deployed to different environments. As such, you will need two CodePipelines. If you had one with a manual approval step at the end, then the code deployed to production would be coming from the master branch instead of the production branch. Here, we specifically need code in the production branch to be deployed to production, so, therefore, we need a second CodePipeline and to merge code from master to production through Pull Requests.
   - `Change the runOrder of your actions so that they have the same value`. You can use the runOrder to specify parallel actions and use the same integer for each action you want to run in parallel. The default runOrder value for an action is 1. The value must be a positive integer (natural number). You cannot use fractions, decimals, negative numbers, or zero. Here, you need to specify a common runOrder value in your CloudFormation template so that all the stage actions happen in parallel.
   - `Launch the EC2 instances on Dedicated Hosts and create a tag for the application. Deploy an AWS Config custom rule backed by a Lambda function that will check the application tag and ensure the instance is launched on the correct launch mode`. An Amazon EC2 Dedicated Host is a physical server with EC2 instance capacity fully dedicated for your use. When you bring your own software licenses from other vendors to Amazon EC2 Dedicated Hosts, you can let AWS take care of all these administrative tasks on your behalf. AWS gives administrators the option to perform a one-time onboarding set up in AWS License Manager. For the given use-case, you need to create a Config custom rule that will check the application tag and ensure the instance is launched as a Dedicated Host.
   - `Set an instance in Standby right after it has launched`. **note**: asg lifecycle hooks may work, but for troubleshooting, it may be enough since it only has one-hour timeout. The default health checks for an Auto Scaling group are EC2 status checks only. If you configure the Auto Scaling group to use ELB health checks, it considers the instance unhealthy if it fails either the EC2 status checks or the ELB health checks. You can put an instance that is in the InService state into the Standby state, update or troubleshoot the instance, and then return the instance to service. Instances that are on standby are still part of the Auto Scaling group, but they do not actively handle application traffic.
   - `Enable the IAM Capability on the CodePipeline configuration for the Deploy CloudFormation stage action`. For the given use-case, `InsufficientCapabilitiesException` means that the CloudFormation stack is trying to create an IAM role but it doesn't have those specified capabilities. As such it must be configured in CodePipeline configuration for the Deploy CloudFormation stage action.
   - `Add a new IAM policy attached to the group to Deny codecommit:GitPush with a condition on the master branch`. **note**: we cannot modify aws managed policies, but we can add more policies to the user groups. For the given use-case, you need to add an extra policy with an explicit Deny. Please note an Explicit Deny always has priority over anything else.
   - `Create an AWS Config managed rule checking for EBS volume encryption. Use a CloudWatch Event rule to provide alerting`. SNS topics when directly integrated with Config can only be used to stream all the notifications and configuration changes and NOT selectively for a given rule. AWS Config has a managed rule to check for EBS volume encryption. For the given use-case, you need to isolate alerts for this managed rule, so you have to use CloudWatch Events which can then have a specific SNS topic as a target for alerting.
   - `The S3 bucket contains files and therefore cannot be deleted by CloudFormation. Create an additional Custom Resource backed by a Lambda function that performs a clean-up of the bucket`. In a CloudFormation template, you can use the AWS::CloudFormation::CustomResource or Custom::String resource type to specify custom resources. Custom resources provide a way for you to write custom provisioning logic in CloudFormation template and have CloudFormation run it during a stack operation, such as when you create, update or delete a stack.
   - `Store the Golden AMI id in an SSM Parameter Store parameter. Create a CloudFormation parameter that points to the SSM Parameter Store, and is passed on to the configuration of the Elastic Beanstalk environment. Create a CloudWatch Event rule that is triggered every week that will launch a Lambda function. That Lambda function should trigger a refresh of all the CloudFormation templates using the UpdateStack API`. For the given use-case, by having the CloudFormation parameters directly pointing at SSM Parameter Store, on any refresh made to the CloudFormation template by the Lambda function which is in turn triggered by CloudWatch events, the template itself will fetch the latest value from the SSM Parameter Store and will apply it accordingly. So this solution is the best fit for the given requirement.
   - `Create a CodeCommit repository and set the CICD pipeline to deploy the master branch. For each new feature being implemented, create a new branch and create pull requests to merge into master. Set an IAM policy on your developer group to prevent direct pushes to master`. **note**: for codeCommit repository, there is no such a thing called 'repository access policy'.
   - `Store the RDS credentials in Secrets Manager and create an IAM instance role for EC2 to access Secrets Manager and DynamoDB`. To access PostgreSQL, you can use database credentials and they're best stored in Secrets Manager from a security best-practices perspective.
   - `Enable Amazon Macie on the selected S3 buckets. Setup alerting using CloudWatch Events`. Amazon Macie is a security service that uses machine learning to automatically discover, classify, and protect sensitive data in AWS. Macie automatically detects a large and growing list of sensitive data types, including personally identifiable information (PII) such as names, addresses, and credit card numbers. It also gives you constant visibility of the data security and data privacy of your data stored in Amazon S3. For the given use-case, you can enable Macie on specific S3 buckets and then configure SNS notifications via CloudWatch events for Macie alerts.
   - `Make CodePipeline deploy to a new Beanstalk environment. After that stage action, create another stage action to invoke a Custom Job using AWS Lambda, which will perform the API call to swap the CNAME of the environments`. When an application is developed and deployed to an AWS Elastic Beanstalk environment, having two separate, but identical, environments — blue and green — increases availability and reduces risk. The blue environment is the production environment that normally handles live traffic. The CI/CD pipeline architecture creates a clone (green) of the live Elastic Beanstalk environment (blue). The pipeline then swaps the URLs between the two environments. While CodePipeline deploys application code to the original environment — and testing and maintenance take place — the temporary clone environment handles the live traffic. Once deployment to the blue environment is successful, and code review and code testing are done, the pipeline again swaps the URLs between the green and blue environments. The blue environment starts serving the live traffic again, and the pipeline terminates the green environment. To perform Blue/Green in Elastic Beanstalk, you need to deploy to a new environment and do a CNAME swap. **The CNAME swap feature is not supported by CloudFormation itself, therefore you need to create a custom Lambda function that will perform that API call for you and invoke it as part of a Custom Job in CodePipeline.**
   - `Create a CloudWatch Alarm on the Lambda CloudWatch metrics and associate it with the CodeDeploy deployment`. You can monitor and automatically react to changes in your AWS CodeDeploy deployments using Amazon CloudWatch alarms. If the alarm is activated, CloudWatch initiates actions such as sending a notification to Amazon Simple Notification Service, stopping a CodeDeploy deployment, or changing the state of an instance (e.g. reboot, terminate, recover). You can also automatically roll back a deployment when a deployment fails or when a CloudWatch alarm is activated.
   - `Create a log destination in the centralized account, and create a log subscription on that destination. Create a Kinesis Firehose delivery stream and subscribe it to the log destination. The target of Kinesis Firehose should be Amazon S3`. You can use subscriptions to get access to a real-time feed of log events from CloudWatch Logs and have it delivered to other services such as an Amazon Kinesis stream, an Amazon Kinesis Data Firehose stream, or AWS Lambda for custom processing, analysis, or loading to other systems. When log events are sent to the receiving service, they are Base64 encoded and compressed with the gzip format. For cross-account log data sharing with subscriptions, you can collaborate with an owner of a different AWS account and receive their log events on your AWS resources, such as an Amazon Kinesis or Amazon Kinesis Data Firehose stream (this is known as cross-account data sharing). For example, this log event data can be read from a centralized Kinesis or Kinesis Data Firehose stream to perform custom processing and analysis. Therefore we have to subscribe the log destination to a Kinesis Firehose delivery stream, which in turn has a destination of S3.
   - `Create a termination hook for your ASG and create a CloudWatch Events rule to trigger an AWS Lambda function. The Lambda function should invoke an SSM Run Command to send the log files from the EC2 instance to S3`. You can use a CloudWatch Events rule to invoke a Lambda function when a lifecycle action occurs. The Lambda function is invoked when Amazon EC2 Auto Scaling submits an event for a lifecycle action to CloudWatch Events. The event contains information about the instance that is launching or terminating and a token that you can use to control the lifecycle action. Finally, the Lambda function can invoke an SSM Run Command to send the log files from the EC2 instance to S3. SSM Run Command lets you remotely and securely manage the configuration of your managed instances.
   - `AutoScalingRollingUpdate`. To specify how AWS CloudFormation handles rolling updates for an Auto Scaling group, use the AutoScalingRollingUpdate policy. Rolling updates enable you to specify whether AWS CloudFormation updates instances that are in an Auto Scaling group in batches or all at once. AutoScalingRollingUpdate is perfect for the given use case. **note**: `AutoScalingReplacingUpdate` will create a new ASG entirely.
   - `Create a new Lambda function that will read from the stream and pass on the payload to SNS. Have the other three and upcoming Lambda functions directly read from the SNS topic`. DynamoDB Streams captures a time-ordered sequence of item-level modifications in any DynamoDB table and stores this information in a log for up to 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in near-real-time. the destinations of dynamodb stream are kinesis data stream, lambda, aws KCL app. If you enable DynamoDB Streams on a table, you can associate the stream Amazon Resource Name (ARN) with a Lambda function that you write. Immediately after an item in the table is modified, a new record appears in the table's stream. AWS Lambda polls the stream and invokes your Lambda function synchronously when it detects new stream records. No more than two processes at most should be reading from the same streams shard at the same time. Having more than two readers per shard can result in throttling. Therefore, you need to use a fan-out pattern for this, SNS being perfect for that.
   - `The target of the rule should be a Lambda function that will invoke a 3rd party Slack webhook`. Here we are only interested in pipeline failures, so we need to choose CodePipeline Pipeline Execution State Change. Finally, **CloudWatch Event rules do not support Slack as a target, therefore we must create a Lambda function for it.**
   - `Remove one ALB and keep the two ASG. When new deployments happen, deploy to the older ASG, and then swap the target group in the ALB rule. Keep the Route53 record pointing to the ALB`. The correct solution is to replace only the infrastructure behind the load balancer. To summarize, we can migrate to one ALB only and then just use one target group at a time behind each ASG for correct routing. This will have the added benefit that we won't need to pre-warm each ALB at each deployment.
   - `Create an AWS Step Function. Implement each step as a Lambda function and add failure logic between the steps to deal with conditional cases`. For the given use-case, you need to combine Step Functions, Lambda and CloudWatch Events into a single coherent solution. You can use the Step Functions to coordinate the business logic to automate the snapshot management flow with error handling, retry logic, and workflow logic all baked into the Step Functions definition. CloudWatch Events integrates with Step Functions and Lambda to let you execute your custom code when relevant events occur.
   - `Install the SSM agent on the instances. Run an SSM Inventory to collect the metadata and send them to Amazon S3`. SSM Inventory provides visibility into your Amazon EC2 and on-premises computing environment. You can use Inventory to collect metadata from your managed instances. You can store this metadata in a central Amazon Simple Storage Service (Amazon S3) bucket, and then use built-in tools to query the data and quickly determine which instances are running the software and configurations required by your software policy, and which instances need to be updated. `Create a CloudWatch Event rule to trigger a Lambda function on an hourly basis. Do a comparison of the instances that are running in EC2 and those tracked by SSM`. Since SSM does not have any native capability to find out which instances are not currently tracked by it, so here we would need to create a custom Lambda function for this and send notifications if any new untracked instances are detected. We can trigger the Lambda function using CloudWatch Events. **note**: Inspector is meant to find security vulnerabilities on EC2 instances, not to get a metadata list of your installed packages.
   - `In the ValidateService hook in appspec.yml, verify the service is properly running. Configure CodeDeploy to rollback on deployment failures. In case the hook fails, then CodeDeploy will rollback`. For the given use-case, you can use ValidateService hook to verify that the deployment was completed successfully. This is the last deployment lifecycle event. You can configure CodeDeploy to rollback if this hook fails.
   - `Create one template per logical element of your infrastructure. Deploy them using CloudFormation as they are ready. Use outputs and exports to reference values in the stacks. Keep each file separately in a version-controlled repository`. In CloudFormation the best practice is to separate stacks into individual, separate logical components that have dependencies on each other. To link through these dependencies, the best is to use Exports and Imports. Each individual CloudFormation template must be a separate file. for the given use case, different teams are working on different pieces of the infrastructure with their own timelines, so it's difficult to combine all elements of the infrastructure into a single master template. It's much better to have one template per logical element of the infrastructure that is owned by the respective team and then use outputs and exports to reference values in the stacks.
   - `Enable Trusted Advisor and ensure the check for low-utilized EC2 instances are on. Create a CloudWatch Event that tracks the events created by Trusted Advisor and use a Lambda Function as a target for that event. The Lambda function should trigger an SSM Automation document with a manual approval step. Upon approval, the SSM document proceeds with the instance termination`. Trusted Advisor inspects your AWS infrastructure across all AWS Regions, and then presents a summary of check results. It recommends stopping or terminating EC2 instances with low utilization. Trusted Advisor cost optimization check allows you to check EC2 instances that were running at any time during the last 14 days and alerts you if the daily CPU utilization was 10% or less and network I/O was 5 MB or less on 4 or more days. Running instances generate hourly usage charges. Estimated monthly savings are calculated by using the current usage rate for On-Demand Instances and the estimated number of days the instance might be underutilized. You can use Amazon CloudWatch Events to detect and react to changes in the status of Trusted Advisor checks. Then, based on the rules that you create, CloudWatch Events invokes one or more target actions when a check status changes to the value you specify in a rule. Depending on the type of status change, you might want to send notifications, capture status information, take corrective action, initiate events, or take other actions. Finally, SSM Automation can have a manual approval step and terminate instances. **note**: there's no way to use one single cloudwatch alarm to track metrics across all ec2 instances.
   - **Create an `.ebextensions/db-migration.config` file in your code repository and set a `container_commands` block. Set your migration command there and use the `leader_only: true` attribute**. You can use Elastic Beanstalk configuration files (.ebextensions) with your web application's source code to configure your environment and customize the AWS resources that it contains. Configuration files are YAML- or JSON-formatted documents with a .config file extension that you place in a folder named .ebextensions and deploy in your application source bundle. You can use the `option_settings` key to modify the environment configuration. You can choose from general options for all environments and platform-specific options. You may want to customize and configure the software that your application depends on. You can use the `commands` key to execute commands on the EC2 instance. The commands run before the application and web server are set up and the application version file is extracted. You can use the `container_commands` key to execute commands that affect your application source code. Container commands run after the application and web server have been set up and the application version archive has been extracted, but before the application version is deployed. You can use leader_only to only run the command on a single instance, or configure a test to only run the command when a test command evaluates to true.
   - `Create ECS task definitions that include the awslogs driver. Set an IAM instance role on the EC2 instance with the necessary permissions to write to CloudWatch logs`. Here many solutions may work but we're looking for the simplest possible solution. The important thing to remember is that the ECS task definitions can include the awslogs driver and write to CloudWatch Logs natively. But the EC2 instance will be the one writing to CloudWatch, and therefore it must have an EC2 Instance Role with the appropriate permissions to write to CloudWatch. Your Amazon ECS container instances also require logs:CreateLogStream and logs:PutLogEvents permission on the IAM role with which you launch your container instances.
   - `Create the booking workflow in Step Functions. Create an API Gateway stage using a service integration with Step Functions. Secure your API using Cognito`. Amazon API Gateway integrates with AWS Step Functions, allowing you to call Step Functions with APIs that you create to simplify and customize interfaces to your applications. Step Functions makes it easy to coordinate the components of distributed applications and microservices as a series of steps in a visual workflow. For the given use-case, you need to implement the payment workflow using Step Functions. A key reason you need this integration is that AWS Lambda has a max concurrent execution of 1000, while API gateway has a max concurrent execution of 10000. By integrating API Gateway and Step Functions together, you bypass any limit Lambda would have imposed on you. **note**: api gateway has the same concurrency limit as step functions which is 10,000.
   - `In your appspec.yml file, include a BeforeAllowTraffic hook that checks on the completion of the Step Function execution`. The BeforeAllowTraffic hook is used to run tasks before traffic is shifted to the deployed Lambda function version. So for the given use-case, you can use this hook to check that the restructuring task has fully completed and then shift traffic to the newly deployed Lambda function version.
   - `Create a CloudWatch Event checking for AWS_RISK_CREDENTIALS_EXPOSED in the Health Service. Trigger a Step Function workflow that will issue API calls to IAM, CloudTrail, and SNS to achieve the desired requirements`. AWS monitors popular code repository sites for IAM access keys that have been publicly exposed. AWS Health generates an AWS_RISK_CREDENTIALS_EXPOSED event when an IAM access key has been publicly exposed on GitHub. A CloudWatch Events rule further detects this event and invokes a Step Function that orchestrates the automated workflow to delete the exposed IAM access key, and summarize the recent API activity for the exposed key. The workflow will also issue API calls to IAM, CloudTrail, and SNS. The AWS_RISK_CREDENTIALS_EXPOSED is exposed by the Personal Health Dashboard service. (AWS Health provides ongoing visibility into your resource performance and the availability of your AWS services and accounts. You can use AWS Health events to learn how service and resource changes might affect your applications running on AWS.)
   - `Create a bucket policy to create a condition for Denying any request that is "aws:SecureTransport": "false". Encrypt the objects at rest using SSE-S3. Setup Cross-Region Replication`.  if we encrypt using KMS, we may get throttled at 10000 objects per second. SSE-S3 is a better choice in this case.
   - `Deploy Jenkins as a multi-master setup across multiple AZ. Enable the CodeBuild Plugin for Jenkins so that builds are launched as CodeBuild builds`. In the AWS Cloud, a web-accessible application like Jenkins is typically designed for high availability and fault tolerance by spreading instances across multiple AZs and fronting them with an Elastic Load Balancing (ELB) load balancer. You can use the Jenkins plugin for AWS CodeBuild to integrate CodeBuild with your Jenkins build jobs. Instead of sending your build jobs to Jenkins build nodes, you use the plugin to send your build jobs to CodeBuild. This eliminates the need for you to provision, configure, and manage Jenkins build nodes. For the given use-case, Jenkins must be deployed as a multi-master across multi-AZ to be highly available and fault-tolerant. The Jenkins CodeBuild plugin allows to elastically start CodeBuild builds that run a special docker image that works as a Jenkins slave. It allows you to be fully elastic in the cloud with Jenkins, and only pay exactly for the resources you have used.
   - `Create a separate Beanstalk environment that's a worker environment and processes invoices through an SQS queue. The invoices are uploaded into S3 and a reference to it is sent to the SQS by the web tier. The worker tier processes these files. A cron job defined using the cron.yml file will send out the emails`. For the given use-case, the worker tier is used to asynchronously process the invoices from an SQS queue. SQS size limit is 256KB and therefore the files must be uploaded to S3 and a reference to them should be sent to SQS by the web tier. Finally, the cron.yml file must be defined on the worker tier. Using this strategy we have decoupled our processing tier from our web tier, and CPU usage will go down as a result. The worker tier will also be able to easily scale in case many invoices are uploaded.
   - `Create a CloudWatch metric for the maximum CPU utilization of your EC2 instances. Create a CloudWatch Alarm on top of that metric. Create a deployment in CodeDeploy that has rollback enabled, integrated with the CloudWatch alarm`. You can monitor and automatically react to changes in your AWS CodeDeploy deployments using Amazon CloudWatch alarms. Using CloudWatch with CodeDeploy, you can monitor metrics for Amazon EC2 instances or Auto Scaling groups being managed by CodeDeploy and then invoke an action if the metric you are tracking crosses a certain threshold for a defined period of time.
   - `Create a new API Gateway Stage. Enable Canary deployments on the v1 stage. Deploy the new stage to the v1 stage and assign a small amount of traffic to the canary stage. Track metrics data using CloudWatch`. In a canary release deployment, total API traffic is separated at random into a production release and a canary release with a preconfigured ratio. The updated API features are only visible to the canary release. The canary release receives a small percentage of API traffic and the production release takes up the rest. For the given use-case, you must deploy API to a new stage called v1, enable canary deployment on this v1 stage and assign a small amount of traffic to this canary stage.
   - `Use an Amazon Aurora Global Database for the movies table and use Amazon Aurora for the users and movies_watched tables`. Amazon Aurora Global Database is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages. Amazon Aurora Global Database is the correct choice for the given use-case.
   - `Create an EC2 instance with an IAM role giving access to the S3 bucket where CodeDeploy is deploying from. Ensure that the EC2 instance also has the CodeDeploy agent installed. Tag the instance to have it part of a deployment group`. For the given use-case, you can have the CodePipeline chain CodeCommit and CodeDeploy and have the source code available as a zip file in an S3 bucket to be used as a CodePipeline artifact. The EC2 instance must have an IAM role, and not an IAM user, to pull that file from S3. Finally, the EC2 instance must be properly tagged to be part of the correct deployment group and have the CodeDeploy agent installed on it.
   - `Create an AMI that contains the web application. Configure the dynamic part at runtime using an EC2 User Data script`. A golden AMI is an AMI that you standardize through configuration, consistent security patching, and hardening. It also contains agents you approve for logging, security, performance monitoring, etc. For the given use-case, you can also add the web application as part of the golden AMI. You can think of it as an input base AMI for creating a standardized application-specific golden AMI. (The local cache warmup can unfortunately not be improved, as caching is dynamic and data may change over time.)
   - `Create an Auto Scaling Group Lifecycle Hook for the terminate action. Create a CloudWatch Event rule for that lifecycle hook and invoke a Lambda function. The Lambda function should use an SSM Run Command to extract the application logs and store them in S3`. For the given use-case, you can configure a lifecycle hook to invoke the CloudWatch Event rule to trigger a Lambda function that launches an SSM Run Command to extract the application logs and store them in S3. `Enable Access Logs at the Application Load Balancer level`. Access logging is an optional feature of Elastic Load Balancing that is disabled by default. After you enable access logging for your load balancer, Elastic Load Balancing captures the logs and stores them in the Amazon S3 bucket that you specify as compressed files. You can use these access logs to analyze traffic patterns and troubleshoot issues.
   - **note**: CloudWatch Logs agent can only be used for continuous log streaming, and NOT for a one-time log extract to S3.
   - **note**: access logs are enabled at the ALB level and NOT at the target group level.
   - `Create an IAM Service Role for instances to be able to call the AssumeRole operation on the SSM service. Generate an activation code and activation ID for your on-premise servers. Use these credentials to register your on-premise servers. They will appear with the prefix 'mi-' in your SSM console`. Servers and virtual machines (VMs) in a hybrid environment require an IAM role to communicate with the Systems Manager service. The role grants AssumeRole trust to the Systems Manager service. You only need to create a service role for a hybrid environment once for each AWS account. To set up servers and virtual machines (VMs) in your hybrid environment as managed instances, you need to create a managed-instance activation. After you complete the activation, you immediately receive an Activation Code and Activation ID. You specify this Code/ID combination when you install SSM agents on servers and VMs in your hybrid environment. The Code/ID provides secure access to the Systems Manager service from your managed instances. In the Instance limit field, specify the total number of on-premises servers or VMs that you want to register with AWS as part of the activation. This means you don't need to create a unique activation Code/ID for each managed instance.
   - `Deploy to Elastic Beanstalk using a Multi-Docker container configuration. Package each application as a Docker container in ECR`. Elastic Beanstalk supports the deployment of web applications from Docker containers. With Docker containers, you can define your own runtime environment. Elastic Beanstalk can deploy a Docker image and source code to EC2 instances running the Elastic Beanstalk Docker platform. The platform offers multi-container (and single-container) support. A Dockerrun.aws.json file is an Elastic Beanstalk–specific JSON file that describes how to deploy a set of Docker containers as an Elastic Beanstalk application. You can use a Dockerrun.aws.json file for a multi-container Docker environment. Dockerrun.aws.json describes the containers to deploy to each container instance (Amazon EC2 instance that hosts Docker containers) in the environment as well as the data volumes to create on the host instance for the containers to mount.
   - `Create an AWS Automation document to create that AMI in a master account and share the AMI with the other accounts. When a new AMI is created, un-share the previous AMI and share the new one`. The DevOps team needs to provide approved AMIs, This solution uses Amazon EC2 Systems Manager Automation to drive the workflow. After you have an approved AMI, you can distribute the AMI across AWS Regions, and then share it with any other AWS accounts. To do this, you use an Amazon EC2 Systems Manager Automation document that uses an AWS Lambda function to copy the AMIs across a specified list of regions, and then another Lambda function to share this copied AMI with the other accounts. The resulting AMI IDs can be stored in the SSM Parameter Store or Amazon DynamoDB for later consumption. `Create an AWS Config Custom Rule in all the accounts using CloudFormation StackSets. Report the rule's result using an AWS Config aggregation`. AWS Config provides a detailed view of the resources associated with your AWS account, including how they are configured, how they are related to one another, and how the configurations and their relationships have changed over time. For the given use-case, you need to create a Config custom rule to check that only the new AMI is being used and then report the rule's result using an AWS Config aggregation. An aggregator is an AWS Config resource type that collects AWS Config configuration and compliance data from Multiple accounts and multiple regions / Single account and multiple regions. / An organization in AWS Organizations and all the accounts in that organization that have AWS Config enabled.
   - `Create an RDS Read Replica in a CloudFormation template by specifying SourceDBInstanceIdentifier and wait for it to be created. Afterward, upgrade the RDS Read Replica EngineVersion to the next major version. Then promote the Read Replica and use it as your new master database`. You can minimize downtime on an upgrade by using a rolling upgrade using read replicas. Amazon RDS doesn’t fully automate one-click rolling upgrades. However, you can still perform a rolling upgrade by creating a read replica, upgrading the replica by using the property EngineVersion, promoting the replica, and then routing traffic to the promoted replica. If you want to create a Read Replica DB instance, specify the ID of the source DB instance. The SourceDBInstanceIdentifier property determines whether a DB instance is a Read Replica. (remember `EngineVersion`, and if update its value, it will create a new db instance to replace the current one, which is not correct)
   - `When creating a new task definition for your ECS service, ensure to add the sha256 hash in the full image name so that ECS pulls the correct image every time`. Amazon ECS SHA Tracking provides visibility and identification to track where container images are deployed by using task state change events emitted to CloudWatch Events. SHA Tracking is integrated with Amazon ECR, ECS, Fargate and CloudWatch Events to support application lifecycle operations. You can use the IMAGEID property, which is the SHA digest for the Docker image used to start the container.(basically, using image id instead of image name)
   - `Here, the issue is that CloudFormation does not detect a new file has been uploaded to S3 unless one of these parameters change: - S3Bucket - S3Key - S3ObjectVersion`. Changes to a deployment package in Amazon S3 are not detected automatically during stack updates. To update the function code, you need to change the object key or version or 3 bucket in the template.
   - `Use a rolling update with 20% at a time`. AWS Elastic Beanstalk provides several options for how deployments are processed, including deployment policies (All at once, Rolling, Rolling with additional batch, Immutable, and Traffic splitting) and options that let you configure the batch size and health check behavior during deployments. By default, your environment uses all-at-once deployments. If you created the environment with the EB CLI and it's a scalable environment (you didn't specify the --single option), it uses rolling deployments.
   - `In CodeDeploy, create four deployment groups - one for development, one for staging, one for the canary testing instances in production and one for the entire production instances. Create one CodePipeline and chain up these stages together, introducing a manual approval step after the deployment to the canary instances`.
   - deployment-related processess exam alert:
     - A deployment group contains individually tagged Amazon EC2 instances, Amazon EC2 instances in Amazon EC2 Auto Scaling groups, or both.
     - Deployments that use the EC2/On-Premises compute platform manage the way in which traffic is directed to instances by using an in-place or blue/green deployment type. During an in-place deployment, CodeDeploy performs a rolling update across Amazon EC2 instances. During a blue/green deployment, the latest application revision is installed on replacement instances.
     - If you use an EC2/On-Premises compute platform, be aware that blue/green deployments work with Amazon EC2 instances only.
     - You CANNOT use canary, linear, or all-at-once configuration for EC2/On-Premises compute platform.
     - You can manage the way in which traffic is shifted to the updated Lambda function versions during deployment by choosing a canary, linear, or all-at-once configuration.
     - You can deploy an Amazon ECS containerized application as a task set. You can manage the way in which traffic is shifted to the updated task set during deployment by choosing a canary, linear, or all-at-once configuration.
     - Amazon ECS blue/green deployments are supported using both CodeDeploy and AWS CloudFormation. For blue/green deployments through AWS CloudFormation, you don't create a CodeDeploy application or deployment group.
     - Your deployable content and the AppSpec file are combined into an archive file (also known as application revision) and then upload it to an Amazon S3 bucket or a GitHub repository. Remember these two locations. AWS Lambda revisions can be stored in Amazon S3 buckets. EC2/On-Premises revisions are stored in Amazon S3 buckets or GitHub repositories.
     - AWS Lambda and Amazon ECS deployments CANNOT use an in-place deployment type.
     - **note**: An in-place deployment allows you to deploy your application without creating new infrastructure; however, the availability of your application can be affected during these deployments. This approach also minimizes infrastructure costs and management overhead associated with creating new resources.
     - **note**: A blue/green deployment is a deployment strategy in which you create two separate, but identical environments. One environment (blue) is running the current application version and one environment (green) is running the new application version. Using a blue/green deployment strategy increases application availability and reduces deployment risk by simplifying the rollback process if a deployment fails. Once testing has been completed on the green environment, live application traffic is directed to the green environment and the blue environment is deprecated.

- ### practice exam #1 -- 2nd try
  - Add a lifecycle hook that puts the instance in Terminating:Wait status and setup an Amazon CloudWatch event to monitor the Terminating:Wait status. Add an AWS Systems Manager automation document as a CloudWatch Event target. The automation document runs a Windows PowerShell script to remove the instance from the domain and create an AMI of the EC2 instance.
  - Custom resources enable you to write custom provisioning logic in templates that AWS CloudFormation runs anytime you create, update (if you changed the custom resource), or delete stacks. Custom resources require one property: the service token, which specifies where AWS CloudFormation sends requests to, such as an Amazon SNS topic. The custom resource provider processes the AWS CloudFormation request and returns a response of SUCCESS or FAILED to the pre-signed URL. The custom resource provider responds with a JSON-formatted file and uploads it to the pre-signed S3 URL. If this URL is not provided, the calling template will not get an update of the status of the Lambda function and will remain in an in-progress state.
  - Configure AWS Config Auto Remediation for the AWS Config rule s3-bucket-logging-enabled. From the remediation action list choose AWS-ConfigureS3BucketLogging. The AutomationAssumeRole in the remediation action parameters should be assumable by SSM. The user must have pass-role permissions for that role when they create the remediation action in AWS Config.
  - Set up Systems Manager Agent on all instances to manage patching. Test patches in pre-production and then deploy as a maintenance window task with the appropriate approval. Apply patch baselines using the AWS-RunPatchBaseline SSM document.
  - AWS CloudFormation does not support drift detection of custom resources. AWS Config rule depends on the availability of `DetectStackDrift` action of cloudformation api. You receive a throttling or "Rate Exceeded" error because AWS Config defaults the rule to NON_COMPLIANT when throttling occurs.
  - Add a BeforeAllowTraffic hook to the AppSpec file that tests and waits for any necessary database changes before traffic can flow to the new version of the Lambda function.
  - Configure Attribute-based access control by use of tags. Create tags for IAM roles based on their function. Whenever a new resource is created, add these tag(s) to the resource for immediate access to the resource created.
  - Add a reader instance to the Aurora cluster. Update the application configuration to use the Aurora cluster endpoint for write operations. Update the Aurora cluster reader endpoint for read operations. An instance endpoint connects to a specific DB instance within an Aurora cluster. Each DB instance in a DB cluster has its unique instance endpoint. A cluster endpoint (or writer endpoint) for an Aurora DB cluster connects to the current primary DB instance for that DB cluster. This endpoint is the only one that can perform write operations such as DDL statements. The cluster endpoint provides failover support for read/write connections to the DB cluster. During a failover, the DB cluster continues to serve connection requests to the cluster endpoint from the new primary DB instance, with minimal interruption of service. This is the reason we need to change the application configuration to point to cluster endpoint and not to instance endpoint, in the current scenario.
  - Use Scheduled scaling Auto Scaling policy, and create a scheduled action with AWS Lambda function as a scalable target. Specify the minimum and maximum capacity based on the requirements. AWS CLI, SDKs, or CloudFormation can be used for configuring the schedule scaling. **note**: Step scaling policies are not supported for DynamoDB, Amazon Comprehend, Lambda, Amazon Keyspaces, Amazon MSK, ElastiCache, or Neptune.
  - Launch a CloudFormation stack that deploys all the resources of the stack. Add a Retain attribute to the deletion policy of each of these resources. Delete the original stack. Create a new stack with a different name and import the resources that were retained from the original stack. Remove the Retain attribute from the stack to revert to the original template. **note**: 'Snapshot' attribute will still delete the resource after taking its snapshot.
  - Enable server access logging on the S3 bucket. Configure Amazon Athena to create an external table with the log files. Use SQL query to analyze the access patterns from Athena. Server access logging provides detailed records for the requests that are made to an Amazon S3 bucket. Server access logs are useful for many applications. Amazon S3 periodically collects access log records, consolidates the records in log files, and then uploads log files to your target bucket as log objects.
  - Create a CloudWatch Logs subscription to deliver the login event data of Amazon EC2 instances to an AWS Lambda function. Configure the Lambda function to add a decommission tag to the EC2 instance that produced the login event. Schedule an Amazon EventBridge rule to invoke another Lambda function every hour, to terminate all EC2 instances with the decommission tag.
  - Utilize EC2 Image Builder to rebuild the custom AMI that includes the latest AWS Systems Manager Agent version. Set up the Auto Scaling group to attach the AmazonSSMManagedInstanceCore role to EC2 instances. Leverage Systems Manager Session Manager for centralized and automated login. Configure logging of session details to Amazon S3. Set up an S3 event notification for new file uploads to notify the security team via an Amazon Simple Notification Service (Amazon SNS) topic.
  - Use Amazon CloudWatch cross-account observability to set up a security and operations account as the monitoring account and link it with the rest of the member accounts of the organization using AWS Organizations. With Amazon CloudWatch cross-account observability, you can monitor and troubleshoot applications that span multiple accounts within a Region. Seamlessly search, visualize, and analyze your metrics, logs, and traces in any of the linked accounts without account boundaries.
  - Use Serverless Application Model (SAM) and leverage the built-in traffic-shifting feature of SAM to deploy the new Lambda version via CodeDeploy and use pre-traffic and post-traffic test functions to verify code. Rollback in case CloudWatch alarms are triggered. To address the given use case, you can use the traffic shifting feature of SAM to easily test the new version of the Lambda function without having to manually move 100% of the traffic to the new version in one shot.
  - Provide the MD5 digest within the Content-MD5 parameter of the PUT command. Examine the Amazon S3's call return status to check for an error. Examine the returned response for the ETag. Compare the ETag value of the object with a calculated or previously stored Content-MD5 digest.
  - Set up AWS Config in the AWS account. Use a managed rule for Resource Type `EC2::Volume` that returns a compliance failure if the custom tag is not applied to the EBS volume. Configure a remediation action that uses custom AWS Systems Manager Automation documents (runbooks) to apply the custom tag with predefined backup frequency to all non-compliant EBS volumes.
  - From the management account of AWS Organizations, create an AWS CloudFormation stack set to enable AWS Config and deploy your centralized AWS Identity and Access Management (IAM) roles. Configure the stack set to deploy automatically when an account is created through AWS Organizations. You can centrally orchestrate any AWS CloudFormation enabled service across multiple AWS accounts and regions. CloudFormation StackSets simplify the configuration of cross-account permissions and allow for the automatic creation and deletion of resources when accounts are joined or removed from your Organization.
  - Set up an Amazon EventBridge rule that matches an AWS Config evaluation result of NON_COMPLIANT for the restricted-ssh rule. Create an input transformer for the EventBridge rule. Set up the EventBridge rule to publish a notification to the SNS topic.
  - Set up AWS CloudTrail to deliver the API logs to CloudWatch Logs. Leverage the Amazon CloudWatch Agent to deliver logs from the EC2 instances to Amazon CloudWatch Logs. Utilize the CloudWatch Logs Insights to query both sets of logs.
  - **note**: cloudwatch agent cannot send logs from ec2 instances to s3 bucket. cloudtrail can be configured to send logs to cloudwatch logs as well as s3 bucket
  - Set up the Amazon CloudWatch Agent on each Amazon EC2 instance with the configuration to push all logs to Amazon CloudWatch Logs. Create a CloudWatch metric filter to detect user logins. Use Amazon SNS to notify the security team when a login is detected. The Amazon CloudWatch agent can be installed on EC2 instances to collect and send log data to CloudWatch Logs. By setting up a metric filter within CloudWatch Logs, it is possible to search for specific patterns, such as user login events. If a user login is found in the log data, Amazon SNS is used to send an immediate notification to the security team.
  - **note**: A subscription filter defines the filter pattern to use for filtering which logs events get delivered to your AWS resource, as well as information about where to send matching log events to. while metric Filters (not subscription filters) to turn log data into Amazon CloudWatch Metrics for graphing or alarming.
  - Create a VPC peering connection to connect Account A to Account B. Update the EFS file system policy to provide Account B with access to mount and write to the EFS file system in Account A. Update the Lambda execution roles with permission to access the VPC and the EFS file system. `Execution role and user permissions`: If the file system doesn't have a user-configured AWS Identity and Access Management (IAM) policy, EFS uses a default policy that grants full access to any client that can connect to the file system using a file system mount target. If the file system has a user-configured IAM policy, your function's execution role must have the correct elasticfilesystem permissions. `Configuring a file system and access point`: To connect an EFS file system with a Lambda function, you use an EFS access point, an application-specific entry point into an EFS file system that includes the operating system user and group to use when accessing the file system, file system permissions, and can limit access to a specific path in the file system. This helps keep file system configuration decoupled from the application code. `You can access the same EFS file system from multiple functions, using the same or different access points.` Connecting to a file system: A function connects to a file system over the local network in a VPC. The subnets that your function connects to can be the same subnets that contain mount points for your file system, or subnets in the same Availability Zone that can route NFS traffic (port 2049) to the file system. To mount an EFS file system, your Lambda functions must be connected to an Amazon Virtual Private Cloud (Amazon VPC) that can reach the EFS mount targets.
  - `Configure Amazon EventBridge events for AWS Glue. Define the AWS Lambda function as a target to the EventBridge. The Lambda function will have the logic to process the events and filter the AWS Glue job retry failure event. Publish a message to Amazon Simple Notification Service (Amazon SNS) notification if such an event is found.` Amazon EventBridge events for AWS Glue can be used to create Amazon SNS alerts, but the alerts might not be specific enough for certain situations. To receive SNS notifications for certain AWS Glue Events, such as an AWS Glue job failing on retry, you can use AWS Lambda. (**note**: eventbridge does not have a specific event for failing a retry of glue job)
  - Set up a CodePipeline action that runs immediately after the API deployment stage. Configure this action to invoke an AWS Lambda function. The Lambda function will then download the SDK from API Gateway, upload it to the S3 bucket, and create a CloudFront invalidation for the SDK path.
  - Initiate a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment's EC2 instances. Once the rolling restart is complete, leverage an AWS CLI command to update the ALB and direct traffic to the green environment's target group.
  - In Account A, create an encrypted AMI from the unencrypted version and specify the KMS key in the copy action. Modify the key policy to give permissions to Account B for creating a grant. Share the encrypted AMI with Account B. The encrypted snapshots must be encrypted with a KMS key. You can’t share AMIs that are backed by snapshots that are encrypted with the default AWS-managed key. If you create a customer-managed key in a different account than the Auto Scaling group, you must use a grant in combination with the key policy to allow cross-account access to the key. This is a two-step process (refer to the image attached): 1.The first policy allows Account A to give an IAM user or role in the specified Account B permission to create a grant for the key. However, this does not by itself give any users access to the key. 2.Then, from Account B which contains the Auto Scaling group, create a grant that delegates the relevant permissions to the appropriate service-linked role. The Grantee Principal element of the grant is the ARN of the appropriate service-linked role. The key-id is the ARN of the key.
  - Enable logging and provide an Amazon S3 bucket ARN as a WAF logging destination. Bucket names for AWS WAF logging must start with `aws-waf-logs-` and can end with any suffix you want. To send your web ACL traffic logs to Amazon S3, you need to set up an Amazon S3 bucket for the logs. When you enable logging for AWS WAF, you provide the bucket ARN. Your web ACLs publish their log files to the Amazon S3 bucket at 5-minute intervals. Each log file contains log records for the traffic recorded in the previous 5 minutes.
  - **note**: AWS WAF doesn't support encryption for AWS Key Management Service keys that are managed by AWS.
  - Create a CloudFormation template describing the application infrastructure in the Resources section. Use CloudFormation stack set from an administrator account to launch stack instances that deploy the application to various other regions. For the given use case, you need to create a stack set that includes the CloudFormation template that you want to use to create stacks and then identify the AWS Regions in which you want to deploy stacks in your target accounts. A stack set ensures consistent deployment of the same stack resources, with the same settings, to all specified target accounts within the Regions you choose.
  - Create an AWS KMS key to use with CodePipeline in the development account. Also, the input bucket from the development account must have versioning activated to work with CodePipeline. You must use the AWS Key Management Service (AWS KMS) customer-managed key for cross-account deployments. If the key isn't configured, then CodePipeline encrypts the objects with default encryption, which can't be decrypted by the role in the destination account. The input bucket must have versioning activated to work with CodePipeline.
  - Use AWS CodeDeploy with a deployment type configured to Blue/Green deployment configuration. To terminate the original fleet after two hours, change the deployment settings of the Blue/Green deployment. Set Original instances value to Terminate the original instances in the deployment group and choose a waiting period of two hours.
  - Create a metric filter on Amazon CloudWatch by filtering the log data to match the term Critical from log events. Publish a custom metric for the finding and configure a CloudWatch alarm on this custom metric to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the email address of the security team to the SNS topic.
  - `A new checksum value for the object that is calculated based on the checksum values of the individual parts has been created. This behavior is expected`. When you perform some operations using the AWS Management Console, Amazon S3 uses a multipart upload if the object is greater than 16 MB in size. In this case, the checksum is not a direct checksum of the full object, but rather a calculation based on the checksum values of each individual part. Amazon S3 uses the multipart upload functionality to update the object. As a result, Amazon S3 creates a new checksum value for the object that is calculated based on the checksum values of the individual parts.
  - `When an Auto Scaling group with a mixed instances policy scales in, Amazon EC2 Auto Scaling will first identify which of the two types (Spot or On-Demand) should be terminated. This can temporarily cause a misbalance between the AZs`. When an Auto Scaling group with a mixed instances policy scales in, Amazon EC2 Auto Scaling still uses termination policies to prioritize which instances to terminate, but first it identifies which of the two types (Spot or On-Demand) should be terminated. It then applies the termination policies in each Availability Zone individually.
  - Configure the AWS Lambda function to use provisioned concurrency. Configure the application Auto Scaling on the Lambda function with provisioned concurrency values of 1 (minimum) and 100 (maximum) respectively. For a Lambda function, concurrency is the number of in-flight requests your function is handling at the same time. Provisioned concurrency is the number of pre-initialized execution environments you want to allocate to your function. These execution environments are prepared to respond immediately to incoming function requests. Configuring provisioned concurrency incurs charges to your AWS account. Estimating required provisioned concurrency- If your function is currently serving traffic, you can easily view its concurrency metrics using CloudWatch metrics. Specifically, the ConcurrentExecutions metric shows you the number of concurrent invocations for each function in your account. When working with provisioned concurrency, Lambda suggests including a 10% buffer on top of the amount of concurrency your function typically needs.
  - In account A, create a customer-managed AWS KMS key that grants usage permissions to account A's CodePipeline service role and account B. Also, create an Amazon Simple Storage Service (Amazon S3) bucket with a bucket policy that grants account B access to the bucket. In account B, create a cross-account IAM role. In account A, add the AssumeRole permission to account A's CodePipeline service role to allow it to assume the cross-account role in account B. In account B, create a service role for the CloudFormation stack that includes the required permissions for the services deployed by the stack. In account A, update the CodePipeline configuration to include the resources associated with account B.
  - Establish a trust relationship in the application account's deployment IAM role for the centralized DevOps account, allowing the sts:AssumeRole action. Also, grant the application account's deployment IAM role the necessary access to the EKS cluster. Additionally, configure the EKS cluster aws-auth ConfigMap to map the role to the appropriate system permissions.
  - Configure Amazon Route 53 to point to API Gateway APIs in Europe and Asia-Pacific regions. Use latency-based routing and health checks in Route 53. Configure the APIs to forward requests to an AWS Lambda function in the respective Regions. Setup the Lambda function to retrieve and update the data in a DynamoDB global table.
  - Configure Amazon Inspector to detect vulnerabilities on the EC2 instances. Install the Amazon CloudWatch Agent to capture system logs and record them via Amazon CloudWatch Logs. Configure your trail to send log events to CloudWatch Logs. Amazon Inspector is an automated vulnerability management service that continually scans Amazon Elastic Compute Cloud (EC2), AWS Lambda functions, and container workloads for software vulnerabilities and unintended network exposure. To successfully scan Amazon EC2 instances for software vulnerabilities, Amazon Inspector requires that these instances have the SSM agent installed. Amazon Inspector uses SSM Agent to collect application inventory, which can be set up as Amazon Virtual Private Cloud (VPC) endpoints to avoid sending information over the internet.
  - AWS Firewall Manager offers the freedom to use multiple AWS accounts and to host applications in any desired region while maintaining centralized control over their organization’s security settings and profile. Developers can develop and innovators can innovate, while the security team gains the ability to respond quickly, uniformly, and globally to potential threats and actual attacks. Firewall Manager is built around named policies that contain WAF rule sets and optional AWS Shield advanced protection. Each policy applies to a specific set of AWS resources, specified by account, resource type, resource identifier, or tag. Policies can be applied automatically to all matching resources, or to a subset that you select. Firewall Manager has three prerequisites: aws organizations, firewall administrator, aws config
  - Opt for a pilot light DR strategy. Provision a copy of your core workload infrastructure to a different AWS Region. Create an RDS read replica in the new Region, and configure the new environment to point to the local RDS PostgreSQL DB instance. Configure Amazon Route 53 health checks to automatically initiate DNS failover to a new Region. Promote the read replica to the primary DB instance in case of a disaster.








 




















































